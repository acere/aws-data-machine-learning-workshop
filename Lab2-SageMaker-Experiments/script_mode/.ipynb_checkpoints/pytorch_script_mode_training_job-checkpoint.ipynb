{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7e4dca-3653-4bf6-80a0-d964492d1d91",
   "metadata": {},
   "source": [
    "# Track an experiment while training a Pytorch model with a SageMaker Training Job\n",
    "\n",
    "This notebook shows how you can use the SageMaker SDK to track a Machine Learning experiment using a Pytorch model trained in a SageMaker Training Job with Script mode, where you will provide the model script file.\n",
    "\n",
    "We introduce two concepts in this notebook -\n",
    "\n",
    "* *Experiment:* An experiment is a collection of runs. When you initialize a run in your training loop, you include the name of the experiment that the run belongs to. Experiment names must be unique within your AWS account. \n",
    "* *Run:* A run consists of all the inputs, parameters, configurations, and results for one iteration of model training. Initialize an experiment run for tracking a training job with Run(). \n",
    "\n",
    "\n",
    "To execute this notebook in SageMaker Studio, you should select the `PyTorch 1.12 Python 3.8 CPU Optimizer` image.\n",
    "\n",
    "\n",
    "You can track artifacts for experiments, including datasets, algorithms, hyperparameters and metrics. Experiments executed on SageMaker such as SageMaker training jobs are automatically tracked and any existen SageMaker experiment on your AWS account is automatically migrated to the new UI version.\n",
    "\n",
    "In this notebook we will demonstrate the capabilities through an MNIST handwritten digits classification example. The notebook is organized as follow:\n",
    "\n",
    "1. Train a Convolutional Neural Network (CNN) Model and log the model training metrics\n",
    "1. Tune the hyperparameters that configures the number of hidden channels and the optimized in the model. Track teh parameter's configuration, resulting model loss and accuracy and automatically plot a confusion matrix using the Experiments capabilities of the SageMaker SDK.\n",
    "1. Analyse your model results and plot graphs comparing your model different runs generated from the tunning step 3.\n",
    "\n",
    "## Runtime\n",
    "This notebook takes approximately 45 minutes to run.\n",
    "\n",
    "## Contents\n",
    "1. [Install modules](#Install-modules)\n",
    "1. [Setup](#Setup)\n",
    "1. [Create model training script](#Create-model-training-script)\n",
    "1. [Train model with Run context](#Train-model-with-Run-context)\n",
    "1. [Contact](#Contact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1141d3f8-45ed-4a56-8651-8964446befac",
   "metadata": {},
   "source": [
    "## Install modules\n",
    "\n",
    "Let's ensure we have the latest SageMaker SDK available, including the SageMaker Experiments functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d538673-0c04-455a-83a4-157d72edd3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba6534d0-316b-4227-af84-37349d39c81b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (22.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: boto3 in /opt/conda/lib/python3.8/site-packages (1.26.41)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.51-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3) (0.10.0)\n",
      "Collecting botocore<1.30.0,>=1.29.51\n",
      "  Downloading botocore-1.29.51-py3-none-any.whl (10.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.51->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.51->boto3) (1.26.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.51->boto3) (1.16.0)\n",
      "Installing collected packages: botocore, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.41\n",
      "    Uninstalling botocore-1.29.41:\n",
      "      Successfully uninstalled botocore-1.29.41\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.41\n",
      "    Uninstalling boto3-1.26.41:\n",
      "      Successfully uninstalled boto3-1.26.41\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.22.68 requires botocore==1.24.13, but you have botocore 1.29.51 which is incompatible.\n",
      "awscli 1.22.68 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.26.51 botocore-1.29.51\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.126.0)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.128.0.tar.gz (660 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.51)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.22.2)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.19.4)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.11.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.4.1)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.51 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.51)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->sagemaker) (3.0.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.51->boto3<2.0,>=1.26.28->sagemaker) (1.26.8)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.128.0-py2.py3-none-any.whl size=897013 sha256=be2d2133d12313a2cde9cc00eed9aa9d43da4f5af731377fe59b4da363109c2d\n",
      "  Stored in directory: /root/.cache/pip/wheels/28/74/52/e4a4bf559df2f84011ae0f49006eebd0778fed20337d682c43\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.126.0\n",
      "    Uninstalling sagemaker-2.126.0:\n",
      "      Successfully uninstalled sagemaker-2.126.0\n",
      "Successfully installed sagemaker-2.128.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (1.10.2+cpu)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch) (4.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (0.11.3+cpu)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision) (1.22.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from torchvision) (1.10.2+cpu)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->torchvision) (4.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# update boto3 and sagemaker to ensure latest SDK version\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade boto3\n",
    "!{sys.executable} -m pip install --upgrade sagemaker\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368d208-aebb-4844-bf27-2b2e373ef3d2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and set logging and experiment configuration\n",
    "\n",
    "SageMaker Experiments now provides the `Run` class that allows you to create a new experiment run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "037c2813-b191-4420-b37b-9c6d1cbb8057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training-job-experiment-1672305742-bcbe\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.experiments.run import Run, load_run\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import unique_name_from_base\n",
    "\n",
    "role = get_execution_role()\n",
    "region = Session().boto_session.region_name\n",
    "\n",
    "\n",
    "# set new experiment configuration\n",
    "#experiment_name = unique_name_from_base(\"training-job-experiment\")\n",
    "experiment_name=\"training-job-experiment-1672305742-bcbe\"\n",
    "run_name = \"experiment-run-example\"\n",
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc0054-d7dd-4ec8-b1e9-0b292fc7b1c0",
   "metadata": {},
   "source": [
    "## Create model training script\n",
    "Let's create `mnist.py`, the pytorch script file to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49259885-530e-4675-bd72-e21934014e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20c6e08a-92d3-4819-a080-4858337813cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/mnist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./script/mnist.py\n",
    "# ensure that the latest version of the SageMaker SDK is available\n",
    "import os\n",
    "\n",
    "os.system(\"pip install -U sagemaker\")\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from os.path import join\n",
    "import boto3\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.experiments.run import load_run\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "if \"SAGEMAKER_METRICS_DIRECTORY\" in os.environ:\n",
    "    log_file_handler = logging.FileHandler(\n",
    "        join(os.environ[\"SAGEMAKER_METRICS_DIRECTORY\"], \"metrics.json\")\n",
    "    )\n",
    "    formatter = logging.Formatter(\n",
    "        \"{'time':'%(asctime)s', 'name': '%(name)s', \\\n",
    "        'level': '%(levelname)s', 'message': '%(message)s'}\",\n",
    "        style=\"%\",\n",
    "    )\n",
    "    log_file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(log_file_handler)\n",
    "\n",
    "# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, kernel_size, drop_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, hidden_channels, kernel_size=kernel_size)\n",
    "        self.conv2 = torch.nn.Conv2d(hidden_channels, 20, kernel_size=kernel_size)\n",
    "        self.conv2_drop = torch.nn.Dropout2d(p=drop_out)\n",
    "        self.fc1 = torch.nn.Linear(320, 50)\n",
    "        self.fc2 = torch.nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.nn.functional.relu(\n",
    "            torch.nn.functional.max_pool2d(self.conv2_drop(self.conv2(x)), 2)\n",
    "        )\n",
    "        x = x.view(-1, 320)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return torch.nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def log_performance(model, data_loader, device, epoch, run, metric_type=\"Test\"):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss += torch.nn.functional.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(data_loader.dataset)\n",
    "    # log metrics\n",
    "    run.log_metric(name=metric_type + \":loss\", value=loss, step=epoch)\n",
    "    run.log_metric(name=metric_type + \":accuracy\", value=accuracy, step=epoch)\n",
    "    logger.info(\n",
    "        \"{} Average loss: {:.4f}, {} Accuracy: {:.4f}%;\\n\".format(\n",
    "            metric_type, loss, metric_type, accuracy\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    run, train_set, test_set, data_dir=\"mnist_data\", optimizer=\"sgd\", epochs=10, hidden_channels=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Function that trains the CNN classifier to identify the MNIST digits.\n",
    "    Args:\n",
    "        run (sagemaker.experiments.run.Run): SageMaker Experiment run object\n",
    "        train_set (torchvision.datasets.mnist.MNIST): train dataset\n",
    "        test_set (torchvision.datasets.mnist.MNIST): test dataset\n",
    "        data_dir (str): local directory where the MNIST datasource is stored\n",
    "        optimizer (str): the optimization algorthm to use for training your CNN\n",
    "                         available options are sgd and adam\n",
    "        epochs (int): number of complete pass of the training dataset through the algorithm\n",
    "        hidden_channels (int): number of hidden channels in your model\n",
    "    \"\"\"\n",
    "\n",
    "    # log the parameters of your model\n",
    "    run.log_parameter(\"device\", \"cpu\")\n",
    "    run.log_parameters(\n",
    "        {\n",
    "            \"data_dir\": data_dir,\n",
    "            \"optimizer\": optimizer,\n",
    "            \"epochs\": epochs,\n",
    "            \"hidden_channels\": hidden_channels,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # train the model on the CPU (no GPU)\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # set the seed for generating random numbers\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=1000, shuffle=True)\n",
    "    logger.info(\n",
    "        \"Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "            len(train_loader.sampler),\n",
    "            len(train_loader.dataset),\n",
    "            100.0 * len(train_loader.sampler) / len(train_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        \"Processes {}/{} ({:.0f}%) of test data\".format(\n",
    "            len(test_loader.sampler),\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * len(test_loader.sampler) / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "    model = Net(hidden_channels, kernel_size=5, drop_out=0.5).to(device)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    momentum = 0.5\n",
    "    lr = 0.01\n",
    "    log_interval = 100\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Training Epoch:\", epoch)\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = torch.nn.functional.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                logger.info(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)], Train Loss: {:.6f};\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.sampler),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "        log_performance(model, train_loader, device, epoch, run, \"Train\")\n",
    "        log_performance(model, test_loader, device, epoch, run, \"Test\")\n",
    "    # log confusion matrix\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            run.log_confusion_matrix(target, pred, \"Confusion-Matrix-Test-Data\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    hidden_channels = int(os.environ.get(\"hidden_channels\", \"5\"))\n",
    "    kernel_size = int(os.environ.get(\"kernel_size\", \"5\"))\n",
    "    dropout = float(os.environ.get(\"dropout\", \"0.5\"))\n",
    "    model = torch.nn.DataParallel(Net(hidden_channels, kernel_size, dropout))\n",
    "    with open(os.path.join(model_dir, \"model.pth\"), \"rb\") as f:\n",
    "        #model.load_state_dict(torch.load(f))\n",
    "        model = torch.load(model_dir)\n",
    "        model.eval()\n",
    "        return model.to(device)\n",
    "\n",
    "\n",
    "def save_model(model, model_dir, run):\n",
    "    logger.info(\"Saving the model.\")\n",
    "    path = os.path.join(model_dir, \"model.pth\")\n",
    "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
    "    #torch.save(model.cpu().state_dict(), path)\n",
    "    torch.save(model, path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 10)\",\n",
    "    )\n",
    "    parser.add_argument(\"--optimizer\", type=str, default=\"sgd\", help=\"optimizer for training.\")\n",
    "    parser.add_argument(\n",
    "        \"--hidden_channels\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"number of channels in hidden conv layer\",\n",
    "    )\n",
    "    parser.add_argument(\"--region\", type=str, default=\"us-east-2\", help=\"SageMaker Region\")\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n",
    "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    # download the dataset\n",
    "    # this will not only download data to ./mnist folder, but also load and transform (normalize) them\n",
    "    datasets.MNIST.urls = [\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz\",\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz\",\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz\",\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz\",\n",
    "    ]\n",
    "    train_set = datasets.MNIST(\n",
    "        \"mnist_data\",\n",
    "        train=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    test_set = datasets.MNIST(\n",
    "        \"mnist_data\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    session = Session(boto3.session.Session(region_name=args.region))\n",
    "    with load_run(sagemaker_session=session) as run:\n",
    "        run.log_parameters(\n",
    "            {\"num_train_samples\": len(train_set.data), \"num_test_samples\": len(test_set.data)}\n",
    "        )\n",
    "        for f in os.listdir(train_set.raw_folder):\n",
    "            print(\"Logging\", train_set.raw_folder + \"/\" + f)\n",
    "            run.log_file(train_set.raw_folder + \"/\" + f, name=f, is_output=False)\n",
    "        model = train_model(\n",
    "            run,\n",
    "            train_set,\n",
    "            test_set,\n",
    "            data_dir=\"mnist_data\",\n",
    "            optimizer=args.optimizer,\n",
    "            epochs=args.epochs,\n",
    "            hidden_channels=args.hidden_channels,\n",
    "        )\n",
    "        save_model(model, args.model_dir, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342b174-1d33-4b27-a7cd-28571f1a1507",
   "metadata": {},
   "source": [
    "The cell above saves the `mnist.py` file to our script folder. The file implements the code necessary to train our PyTorch model in SageMaker, using the SageMaker PyTorch image. It uses the `load_run` function to automatically detect the experiment configuration and `run.log_parameter`, `run.log_parameters`, `run.log_file`, `run.log_metric` and `run.log_confusion_matrix` to track the model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1913d9-fe5f-4cf1-aca6-c6f6c12bd21c",
   "metadata": {},
   "source": [
    "## Train model with Run context\n",
    "\n",
    "Let's now train the model with passing the experiement run context to the training job\n",
    "\n",
    "For detailed explanation of API run, refer to source code [here](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/experiments/run.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ebf6145-ed87-4e35-a16f-8d0cc907bafb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiment-run-example-1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name+\"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f266e0-d73d-452c-a3ed-51b0fc48075d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.experiments.run:The run (experiment-run-example) under experiment (training-job-experiment-1672305742-bcbe) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-01-03-07-54-52-337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03 07:54:53 Starting - Starting the training job...\n",
      "2023-01-03 07:55:09 Starting - Preparing the instances for training.........\n",
      "2023-01-03 07:56:41 Downloading - Downloading input data.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:32,359 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:32,361 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:32,362 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:32,374 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:32,376 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:32,549 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:32,551 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:32,561 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:32,563 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:32,573 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:32,575 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:32,584 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_channels\": 5,\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"region\": \"ap-southeast-2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-01-03-07-54-52-337\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-07-54-52-337/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"adam\",\"region\":\"ap-southeast-2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-07-54-52-337/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.c5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"adam\",\"region\":\"ap-southeast-2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-01-03-07-54-52-337\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-07-54-52-337/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_channels\",\"5\",\"--optimizer\",\"adam\",\"--region\",\"ap-southeast-2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_CHANNELS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=ap-southeast-2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region ap-southeast-2\u001b[0m\n",
      "\u001b[34m2023-01-03 07:57:33,094 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.122.0)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.126.0.tar.gz (654 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 654.9/654.9 kB 8.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.30 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2022.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.30->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sagemaker: filename=sagemaker-2.126.0-py2.py3-none-any.whl size=890069 sha256=1573d238c14b91dbbb20197c2bc1ec22528d16588ad15dd7b2a505c97be40758\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/8d/05/18/aed11de4cbba829d865b14cc3758e697e789af1ac01245e290\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.122.0\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.122.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.122.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-2.126.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 9912422/9912422 [00:00<00:00, 426325727.22it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/28881 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 28881/28881 [00:00<00:00, 27087588.06it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1648877/1648877 [00:00<00:00, 127844783.29it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/4542 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 4542/4542 [00:00<00:00, 55059331.70it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.experiments.run:The run (experiment-run-example) under experiment (training-job-experiment-1672305742-bcbe) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mTraining Epoch: 1\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.281 algo-1:43 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.421 algo-1:43 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.422 algo-1:43 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.422 algo-1:43 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.423 algo-1:43 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.423 algo-1:43 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.436 algo-1:43 INFO hook.py:561] name:module.conv1.weight count_params:125\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.436 algo-1:43 INFO hook.py:561] name:module.conv1.bias count_params:5\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.436 algo-1:43 INFO hook.py:561] name:module.conv2.weight count_params:2500\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.436 algo-1:43 INFO hook.py:561] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.436 algo-1:43 INFO hook.py:561] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.436 algo-1:43 INFO hook.py:561] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.436 algo-1:43 INFO hook.py:561] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.436 algo-1:43 INFO hook.py:561] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.436 algo-1:43 INFO hook.py:563] Total Trainable Params: 19210\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.436 algo-1:43 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2023-01-03 07:57:40.438 algo-1:43 INFO hook.py:485] Hook is writing from the hook with pid: 43\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/60000 (11%)], Train Loss: 0.619187;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)], Train Loss: 0.619187;\u001b[0m\n",
      "\n",
      "2023-01-03 07:57:31 Training - Training image download completed. Training in progress.\u001b[34mTrain Epoch: 1 [12800/60000 (21%)], Train Loss: 0.841508;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)], Train Loss: 0.841508;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)], Train Loss: 0.535673;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)], Train Loss: 0.535673;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)], Train Loss: 0.633673;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)], Train Loss: 0.633673;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)], Train Loss: 0.604052;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)], Train Loss: 0.604052;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)], Train Loss: 0.196605;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)], Train Loss: 0.196605;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)], Train Loss: 0.764071;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)], Train Loss: 0.764071;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)], Train Loss: 0.366877;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)], Train Loss: 0.366877;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)], Train Loss: 0.381978;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)], Train Loss: 0.381978;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1360, Train Accuracy: 95.8583%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1360, Train Accuracy: 95.8583%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1266, Test Accuracy: 96.3100%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1266, Test Accuracy: 96.3100%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 2\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)], Train Loss: 0.249367;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)], Train Loss: 0.249367;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)], Train Loss: 0.260965;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)], Train Loss: 0.260965;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)], Train Loss: 0.341665;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)], Train Loss: 0.341665;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)], Train Loss: 0.254568;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)], Train Loss: 0.254568;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)], Train Loss: 0.625225;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)], Train Loss: 0.625225;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)], Train Loss: 0.271208;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)], Train Loss: 0.271208;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)], Train Loss: 0.241313;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)], Train Loss: 0.241313;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)], Train Loss: 0.258134;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)], Train Loss: 0.258134;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)], Train Loss: 0.276261;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)], Train Loss: 0.276261;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1211, Train Accuracy: 96.2967%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1211, Train Accuracy: 96.2967%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1162, Test Accuracy: 96.4100%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1162, Test Accuracy: 96.4100%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 3\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/60000 (11%)], Train Loss: 0.540804;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)], Train Loss: 0.540804;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)], Train Loss: 0.507104;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)], Train Loss: 0.507104;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/60000 (32%)], Train Loss: 0.732960;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)], Train Loss: 0.732960;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)], Train Loss: 0.478505;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)], Train Loss: 0.478505;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32000/60000 (53%)], Train Loss: 0.201122;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)], Train Loss: 0.201122;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)], Train Loss: 0.261213;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)], Train Loss: 0.261213;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/60000 (75%)], Train Loss: 0.312363;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)], Train Loss: 0.312363;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)], Train Loss: 0.468251;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)], Train Loss: 0.468251;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [57600/60000 (96%)], Train Loss: 0.477161;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)], Train Loss: 0.477161;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1195, Train Accuracy: 96.5067%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1195, Train Accuracy: 96.5067%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1105, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 4\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1105, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/60000 (11%)], Train Loss: 0.333059;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)], Train Loss: 0.333059;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)], Train Loss: 0.165329;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)], Train Loss: 0.165329;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/60000 (32%)], Train Loss: 0.313524;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)], Train Loss: 0.313524;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)], Train Loss: 0.170249;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)], Train Loss: 0.170249;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32000/60000 (53%)], Train Loss: 0.286480;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)], Train Loss: 0.286480;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)], Train Loss: 0.191275;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)], Train Loss: 0.191275;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44800/60000 (75%)], Train Loss: 0.423735;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)], Train Loss: 0.423735;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)], Train Loss: 0.274153;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)], Train Loss: 0.274153;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [57600/60000 (96%)], Train Loss: 0.593129;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)], Train Loss: 0.593129;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1136, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1136, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1061, Test Accuracy: 96.7700%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1061, Test Accuracy: 96.7700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 5\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/60000 (11%)], Train Loss: 0.343393;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)], Train Loss: 0.343393;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)], Train Loss: 0.390378;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)], Train Loss: 0.390378;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/60000 (32%)], Train Loss: 0.360940;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)], Train Loss: 0.360940;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)], Train Loss: 0.489606;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)], Train Loss: 0.489606;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32000/60000 (53%)], Train Loss: 0.268219;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)], Train Loss: 0.268219;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)], Train Loss: 0.237594;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)], Train Loss: 0.237594;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44800/60000 (75%)], Train Loss: 0.415231;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)], Train Loss: 0.415231;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)], Train Loss: 0.176550;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)], Train Loss: 0.176550;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [57600/60000 (96%)], Train Loss: 0.283533;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)], Train Loss: 0.283533;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1177, Train Accuracy: 96.5850%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1177, Train Accuracy: 96.5850%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1108, Test Accuracy: 96.8700%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1108, Test Accuracy: 96.8700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 6\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/60000 (11%)], Train Loss: 0.270528;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [6400/60000 (11%)], Train Loss: 0.270528;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/60000 (21%)], Train Loss: 0.321458;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [12800/60000 (21%)], Train Loss: 0.321458;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/60000 (32%)], Train Loss: 0.289076;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [19200/60000 (32%)], Train Loss: 0.289076;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/60000 (43%)], Train Loss: 0.365472;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [25600/60000 (43%)], Train Loss: 0.365472;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [32000/60000 (53%)], Train Loss: 0.418724;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [32000/60000 (53%)], Train Loss: 0.418724;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [38400/60000 (64%)], Train Loss: 0.341385;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [38400/60000 (64%)], Train Loss: 0.341385;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [44800/60000 (75%)], Train Loss: 0.121961;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [44800/60000 (75%)], Train Loss: 0.121961;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/60000 (85%)], Train Loss: 0.361106;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [51200/60000 (85%)], Train Loss: 0.361106;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [57600/60000 (96%)], Train Loss: 0.265346;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [57600/60000 (96%)], Train Loss: 0.265346;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1122, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1122, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1090, Test Accuracy: 96.6700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 7\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1090, Test Accuracy: 96.6700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [6400/60000 (11%)], Train Loss: 0.264878;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [6400/60000 (11%)], Train Loss: 0.264878;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/60000 (21%)], Train Loss: 0.377314;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [12800/60000 (21%)], Train Loss: 0.377314;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19200/60000 (32%)], Train Loss: 0.282708;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [19200/60000 (32%)], Train Loss: 0.282708;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/60000 (43%)], Train Loss: 0.270151;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [25600/60000 (43%)], Train Loss: 0.270151;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [32000/60000 (53%)], Train Loss: 0.463462;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [32000/60000 (53%)], Train Loss: 0.463462;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [38400/60000 (64%)], Train Loss: 0.318820;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [38400/60000 (64%)], Train Loss: 0.318820;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [44800/60000 (75%)], Train Loss: 0.301752;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [44800/60000 (75%)], Train Loss: 0.301752;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [51200/60000 (85%)], Train Loss: 0.491330;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [51200/60000 (85%)], Train Loss: 0.491330;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [57600/60000 (96%)], Train Loss: 0.605309;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [57600/60000 (96%)], Train Loss: 0.605309;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1232, Train Accuracy: 96.2983%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1232, Train Accuracy: 96.2983%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1162, Test Accuracy: 96.6500%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1162, Test Accuracy: 96.6500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 8\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [6400/60000 (11%)], Train Loss: 0.436534;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [6400/60000 (11%)], Train Loss: 0.436534;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/60000 (21%)], Train Loss: 0.697182;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [12800/60000 (21%)], Train Loss: 0.697182;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [19200/60000 (32%)], Train Loss: 0.352988;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [19200/60000 (32%)], Train Loss: 0.352988;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/60000 (43%)], Train Loss: 0.251108;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [25600/60000 (43%)], Train Loss: 0.251108;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [32000/60000 (53%)], Train Loss: 0.337925;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [32000/60000 (53%)], Train Loss: 0.337925;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [38400/60000 (64%)], Train Loss: 0.243655;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [38400/60000 (64%)], Train Loss: 0.243655;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [44800/60000 (75%)], Train Loss: 0.260882;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [44800/60000 (75%)], Train Loss: 0.260882;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [51200/60000 (85%)], Train Loss: 0.439534;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [51200/60000 (85%)], Train Loss: 0.439534;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [57600/60000 (96%)], Train Loss: 0.307441;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [57600/60000 (96%)], Train Loss: 0.307441;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1223, Train Accuracy: 96.1400%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1223, Train Accuracy: 96.1400%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1160, Test Accuracy: 96.4200%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 9\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1160, Test Accuracy: 96.4200%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [6400/60000 (11%)], Train Loss: 0.541070;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [6400/60000 (11%)], Train Loss: 0.541070;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/60000 (21%)], Train Loss: 0.434201;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [12800/60000 (21%)], Train Loss: 0.434201;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [19200/60000 (32%)], Train Loss: 0.197400;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [19200/60000 (32%)], Train Loss: 0.197400;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/60000 (43%)], Train Loss: 0.469770;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [25600/60000 (43%)], Train Loss: 0.469770;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [32000/60000 (53%)], Train Loss: 0.159525;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [32000/60000 (53%)], Train Loss: 0.159525;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [38400/60000 (64%)], Train Loss: 0.757667;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [38400/60000 (64%)], Train Loss: 0.757667;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [44800/60000 (75%)], Train Loss: 0.284950;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [44800/60000 (75%)], Train Loss: 0.284950;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [51200/60000 (85%)], Train Loss: 0.334385;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [51200/60000 (85%)], Train Loss: 0.334385;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [57600/60000 (96%)], Train Loss: 0.203318;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [57600/60000 (96%)], Train Loss: 0.203318;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1043, Train Accuracy: 96.9667%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1043, Train Accuracy: 96.9667%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1033, Test Accuracy: 97.1400%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 10\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1033, Test Accuracy: 97.1400%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [6400/60000 (11%)], Train Loss: 0.277690;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [6400/60000 (11%)], Train Loss: 0.277690;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/60000 (21%)], Train Loss: 0.492950;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [12800/60000 (21%)], Train Loss: 0.492950;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19200/60000 (32%)], Train Loss: 0.284791;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [19200/60000 (32%)], Train Loss: 0.284791;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/60000 (43%)], Train Loss: 0.307791;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [25600/60000 (43%)], Train Loss: 0.307791;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [32000/60000 (53%)], Train Loss: 0.591849;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [32000/60000 (53%)], Train Loss: 0.591849;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [38400/60000 (64%)], Train Loss: 0.527022;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [38400/60000 (64%)], Train Loss: 0.527022;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [44800/60000 (75%)], Train Loss: 0.192943;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [44800/60000 (75%)], Train Loss: 0.192943;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [51200/60000 (85%)], Train Loss: 0.461220;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [51200/60000 (85%)], Train Loss: 0.461220;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [57600/60000 (96%)], Train Loss: 0.206218;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [57600/60000 (96%)], Train Loss: 0.206218;\u001b[0m\n",
      "\n",
      "2023-01-03 08:01:58 Uploading - Uploading generated training model\u001b[34mTrain Average loss: 0.1205, Train Accuracy: 96.3250%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1205, Train Accuracy: 96.3250%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1264, Test Accuracy: 96.4600%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1264, Test Accuracy: 96.4600%;\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34mWARNING:sagemaker.experiments._helper:Skip creating the artifact since it already exists: Artifact arn must be unique within an AWS account and region. Artifact with arn (arn:aws:sagemaker:ap-southeast-2:201364840562:artifact/bef1a108cb5fac6ec94e64f5abc80cd0) already exists.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"mnist.py\", line 262, in <module>\u001b[0m\n",
      "\u001b[34msave_model(model, args.model_dir, run)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/sagemaker/experiments/run.py\", line 710, in __exit__\u001b[0m\n",
      "\u001b[34mself.close()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/sagemaker/experiments/run.py\", line 509, in close\u001b[0m\n",
      "\u001b[34mself._lineage_artifact_tracker.save()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/sagemaker/experiments/_helper.py\", line 266, in save\u001b[0m\n",
      "\u001b[34martifact.add_association(self.sagemaker_session)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/sagemaker/experiments/_helper.py\", line 205, in add_association\u001b[0m\n",
      "\u001b[34msagemaker_session.sagemaker_client.add_association(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/botocore/client.py\", line 530, in _api_call\u001b[0m\n",
      "\u001b[34mreturn self._make_api_call(operation_name, kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/botocore/client.py\", line 919, in _make_api_call\u001b[0m\n",
      "\u001b[34mrequest_dict = self._convert_to_request_dict(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/botocore/client.py\", line 990, in _convert_to_request_dict\u001b[0m\n",
      "\u001b[34mrequest_dict = self._serializer.serialize_to_request(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/botocore/validate.py\", line 381, in serialize_to_request\u001b[0m\n",
      "\u001b[34mraise ParamValidationError(report=report.generate_report())\u001b[0m\n",
      "\u001b[34mbotocore.exceptions\u001b[0m\n",
      "\u001b[34m.ParamValidationError: Parameter validation failed:\u001b[0m\n",
      "\u001b[34mInvalid type for parameter DestinationArn, value: None, type: <class 'NoneType'>, valid types: <class 'str'>\u001b[0m\n",
      "\u001b[34m2023-01-03 08:01:55,020 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-01-03 08:01:55,020 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-01-03 08:01:55,021 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2023-01-03 08:01:55,021 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region ap-southeast-2\"\u001b[0m\n",
      "\u001b[34m2023-01-03 08:01:55,021 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2023-01-03 08:02:09 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the UpdateTrialComponent operation: 1 validation error detected: Value 'Error for Training job pytorch-training-2023-01-03-07-54-52-337: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region ap-southeast-2\", exit code: 1' at 'status.message' failed to satisfy constraint: Member must satisfy regular expression pattern: .*",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   4112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4113\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3643\u001b[0m                 )\n\u001b[0;32m-> 3644\u001b[0;31m             raise exceptions.UnexpectedStatusException(\n\u001b[0m\u001b[1;32m   3645\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2023-01-03-07-54-52-337: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region ap-southeast-2\", exit code: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/experiments/run.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, exc_traceback)\u001b[0m\n\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/experiments/run.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0;31m# Update the trial component with additions from the Run object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_component\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m             \u001b[0;31m# Create Lineage entities for the artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lineage_artifact_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/experiments/trial_component.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;34m\"\"\"Save the state of this TrialComponent to SageMaker.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boto_update_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boto_update_members\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_disassociate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/apiutils/_base_types.py\u001b[0m in \u001b[0;36m_invoke_api\u001b[0;34m(self, boto_method, boto_method_members)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mapi_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_boto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mapi_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboto_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mapi_boto_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mapi_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_boto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_boto_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 )\n\u001b[1;32m    529\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the UpdateTrialComponent operation: 1 validation error detected: Value 'Error for Training job pytorch-training-2023-01-03-07-54-52-337: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region ap-southeast-2\", exit code: 1' at 'status.message' failed to satisfy constraint: Member must satisfy regular expression pattern: .*"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Start training job with experiment setting\n",
    "with Run(experiment_name=experiment_name, run_name=run_name, sagemaker_session=Session()) as run:\n",
    "    estmator = PyTorch(\n",
    "        entry_point=\"./script/mnist.py\",\n",
    "        role=role,\n",
    "        model_dir=False,\n",
    "        framework_version=\"1.12\",\n",
    "        py_version=\"py38\",\n",
    "        instance_type=\"ml.c5.xlarge\",\n",
    "        instance_count=1,\n",
    "        hyperparameters={\"epochs\": 10, \"hidden_channels\": 5, \"optimizer\": \"adam\", \"region\": region},\n",
    "        keep_alive_period_in_seconds=3600,\n",
    "    )\n",
    "\n",
    "    estmator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6ac4846-160a-4381-8ffe-4d545fa6d7fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.experiments.run:The run (experiment-run-example-1) under experiment (training-job-experiment-1672305742-bcbe) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-01-03-08-34-59-185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03 08:35:00 Starting - Starting the training job...\n",
      "2023-01-03 08:35:15 Starting - Preparing the instances for training......\n",
      "2023-01-03 08:36:19 Downloading - Downloading input data...\n",
      "2023-01-03 08:36:55 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,056 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,058 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,059 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,068 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,070 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,260 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,262 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,274 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,276 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,286 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,288 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,297 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_channels\": 5,\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"region\": \"ap-southeast-2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-01-03-08-34-59-185\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-08-34-59-185/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"adam\",\"region\":\"ap-southeast-2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-08-34-59-185/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.c5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"adam\",\"region\":\"ap-southeast-2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-01-03-08-34-59-185\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-08-34-59-185/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_channels\",\"5\",\"--optimizer\",\"adam\",\"--region\",\"ap-southeast-2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_CHANNELS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=ap-southeast-2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region ap-southeast-2\u001b[0m\n",
      "\u001b[34m2023-01-03 08:36:56,841 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.122.0)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.126.0.tar.gz (654 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 654.9/654.9 kB 20.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.30 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2022.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.30->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sagemaker: filename=sagemaker-2.126.0-py2.py3-none-any.whl size=890069 sha256=a72f79e4e567a56ca03e55b5cb193c92ed24f69b4f730fe013a32883f4128788\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/8d/05/18/aed11de4cbba829d865b14cc3758e697e789af1ac01245e290\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.122.0\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.122.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.122.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-2.126.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 9912422/9912422 [00:00<00:00, 221157987.59it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/28881 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 28881/28881 [00:00<00:00, 27455959.62it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1648877/1648877 [00:00<00:00, 120061305.78it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/4542 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 4542/4542 [00:00<00:00, 43996602.24it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.experiments.run:The run (experiment-run-example-1) under experiment (training-job-experiment-1672305742-bcbe) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mTraining Epoch: 1\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.674 algo-1:42 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.840 algo-1:42 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.841 algo-1:42 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.841 algo-1:42 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.842 algo-1:42 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.842 algo-1:42 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.854 algo-1:42 INFO hook.py:561] name:module.conv1.weight count_params:125\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.855 algo-1:42 INFO hook.py:561] name:module.conv1.bias count_params:5\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.855 algo-1:42 INFO hook.py:561] name:module.conv2.weight count_params:2500\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.855 algo-1:42 INFO hook.py:561] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.855 algo-1:42 INFO hook.py:561] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.855 algo-1:42 INFO hook.py:561] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.855 algo-1:42 INFO hook.py:561] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.855 algo-1:42 INFO hook.py:561] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.855 algo-1:42 INFO hook.py:563] Total Trainable Params: 19210\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.855 algo-1:42 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2023-01-03 08:37:04.857 algo-1:42 INFO hook.py:485] Hook is writing from the hook with pid: 42\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/60000 (11%)], Train Loss: 0.619187;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)], Train Loss: 0.619187;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)], Train Loss: 0.841508;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)], Train Loss: 0.841508;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)], Train Loss: 0.535673;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)], Train Loss: 0.535673;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)], Train Loss: 0.633673;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)], Train Loss: 0.633673;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)], Train Loss: 0.604052;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)], Train Loss: 0.604052;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)], Train Loss: 0.196605;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)], Train Loss: 0.196605;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)], Train Loss: 0.764071;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)], Train Loss: 0.764071;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)], Train Loss: 0.366877;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)], Train Loss: 0.366877;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)], Train Loss: 0.381978;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)], Train Loss: 0.381978;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1360, Train Accuracy: 95.8583%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1360, Train Accuracy: 95.8583%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1266, Test Accuracy: 96.3100%;\u001b[0m\n",
      "\u001b[34mTraining Epoch:\u001b[0m\n",
      "\u001b[34m2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1266, Test Accuracy: 96.3100%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)], Train Loss: 0.249367;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)], Train Loss: 0.249367;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)], Train Loss: 0.260965;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)], Train Loss: 0.260965;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)], Train Loss: 0.341665;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)], Train Loss: 0.341665;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)], Train Loss: 0.254568;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)], Train Loss: 0.254568;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)], Train Loss: 0.625225;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)], Train Loss: 0.625225;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)], Train Loss: 0.271208;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)], Train Loss: 0.271208;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)], Train Loss: 0.241313;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)], Train Loss: 0.241313;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)], Train Loss: 0.258134;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)], Train Loss: 0.258134;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)], Train Loss: 0.276261;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)], Train Loss: 0.276261;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1211, Train Accuracy: 96.2967%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1211, Train Accuracy: 96.2967%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1162, Test Accuracy: 96.4100%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 3\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1162, Test Accuracy: 96.4100%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/60000 (11%)], Train Loss: 0.540804;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)], Train Loss: 0.540804;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)], Train Loss: 0.507104;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)], Train Loss: 0.507104;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/60000 (32%)], Train Loss: 0.732960;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)], Train Loss: 0.732960;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)], Train Loss: 0.478505;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)], Train Loss: 0.478505;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32000/60000 (53%)], Train Loss: 0.201122;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)], Train Loss: 0.201122;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)], Train Loss: 0.261213;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)], Train Loss: 0.261213;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/60000 (75%)], Train Loss: 0.312363;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)], Train Loss: 0.312363;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)], Train Loss: 0.468251;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)], Train Loss: 0.468251;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [57600/60000 (96%)], Train Loss: 0.477161;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)], Train Loss: 0.477161;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1195, Train Accuracy: 96.5067%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1195, Train Accuracy: 96.5067%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1105, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 4\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1105, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/60000 (11%)], Train Loss: 0.333059;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)], Train Loss: 0.333059;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)], Train Loss: 0.165329;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)], Train Loss: 0.165329;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/60000 (32%)], Train Loss: 0.313524;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)], Train Loss: 0.313524;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)], Train Loss: 0.170249;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)], Train Loss: 0.170249;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32000/60000 (53%)], Train Loss: 0.286480;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)], Train Loss: 0.286480;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)], Train Loss: 0.191275;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)], Train Loss: 0.191275;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44800/60000 (75%)], Train Loss: 0.423735;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)], Train Loss: 0.423735;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)], Train Loss: 0.274153;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)], Train Loss: 0.274153;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [57600/60000 (96%)], Train Loss: 0.593129;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)], Train Loss: 0.593129;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1136, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1136, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1061, Test Accuracy: 96.7700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 5\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1061, Test Accuracy: 96.7700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/60000 (11%)], Train Loss: 0.343393;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)], Train Loss: 0.343393;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)], Train Loss: 0.390378;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)], Train Loss: 0.390378;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/60000 (32%)], Train Loss: 0.360940;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)], Train Loss: 0.360940;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)], Train Loss: 0.489606;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)], Train Loss: 0.489606;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32000/60000 (53%)], Train Loss: 0.268219;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)], Train Loss: 0.268219;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)], Train Loss: 0.237594;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)], Train Loss: 0.237594;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44800/60000 (75%)], Train Loss: 0.415231;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)], Train Loss: 0.415231;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)], Train Loss: 0.176550;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)], Train Loss: 0.176550;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [57600/60000 (96%)], Train Loss: 0.283533;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)], Train Loss: 0.283533;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1177, Train Accuracy: 96.5850%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1177, Train Accuracy: 96.5850%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1108, Test Accuracy: 96.8700%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1108, Test Accuracy: 96.8700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 6\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/60000 (11%)], Train Loss: 0.270528;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [6400/60000 (11%)], Train Loss: 0.270528;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/60000 (21%)], Train Loss: 0.321458;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [12800/60000 (21%)], Train Loss: 0.321458;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/60000 (32%)], Train Loss: 0.289076;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [19200/60000 (32%)], Train Loss: 0.289076;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/60000 (43%)], Train Loss: 0.365472;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [25600/60000 (43%)], Train Loss: 0.365472;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [32000/60000 (53%)], Train Loss: 0.418724;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [32000/60000 (53%)], Train Loss: 0.418724;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [38400/60000 (64%)], Train Loss: 0.341385;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [38400/60000 (64%)], Train Loss: 0.341385;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [44800/60000 (75%)], Train Loss: 0.121961;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [44800/60000 (75%)], Train Loss: 0.121961;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/60000 (85%)], Train Loss: 0.361106;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [51200/60000 (85%)], Train Loss: 0.361106;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [57600/60000 (96%)], Train Loss: 0.265346;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [57600/60000 (96%)], Train Loss: 0.265346;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1122, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1122, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1090, Test Accuracy: 96.6700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 7\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1090, Test Accuracy: 96.6700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [6400/60000 (11%)], Train Loss: 0.264878;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [6400/60000 (11%)], Train Loss: 0.264878;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/60000 (21%)], Train Loss: 0.377314;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [12800/60000 (21%)], Train Loss: 0.377314;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19200/60000 (32%)], Train Loss: 0.282708;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [19200/60000 (32%)], Train Loss: 0.282708;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/60000 (43%)], Train Loss: 0.270151;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [25600/60000 (43%)], Train Loss: 0.270151;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [32000/60000 (53%)], Train Loss: 0.463462;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [32000/60000 (53%)], Train Loss: 0.463462;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [38400/60000 (64%)], Train Loss: 0.318820;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [38400/60000 (64%)], Train Loss: 0.318820;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [44800/60000 (75%)], Train Loss: 0.301752;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [44800/60000 (75%)], Train Loss: 0.301752;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [51200/60000 (85%)], Train Loss: 0.491330;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [51200/60000 (85%)], Train Loss: 0.491330;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [57600/60000 (96%)], Train Loss: 0.605309;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [57600/60000 (96%)], Train Loss: 0.605309;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1232, Train Accuracy: 96.2983%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1232, Train Accuracy: 96.2983%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1162, Test Accuracy: 96.6500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 8\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1162, Test Accuracy: 96.6500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [6400/60000 (11%)], Train Loss: 0.436534;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [6400/60000 (11%)], Train Loss: 0.436534;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/60000 (21%)], Train Loss: 0.697182;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [12800/60000 (21%)], Train Loss: 0.697182;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [19200/60000 (32%)], Train Loss: 0.352988;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [19200/60000 (32%)], Train Loss: 0.352988;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/60000 (43%)], Train Loss: 0.251108;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [25600/60000 (43%)], Train Loss: 0.251108;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [32000/60000 (53%)], Train Loss: 0.337925;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [32000/60000 (53%)], Train Loss: 0.337925;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [38400/60000 (64%)], Train Loss: 0.243655;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [38400/60000 (64%)], Train Loss: 0.243655;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [44800/60000 (75%)], Train Loss: 0.260882;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [44800/60000 (75%)], Train Loss: 0.260882;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [51200/60000 (85%)], Train Loss: 0.439534;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [51200/60000 (85%)], Train Loss: 0.439534;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [57600/60000 (96%)], Train Loss: 0.307441;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [57600/60000 (96%)], Train Loss: 0.307441;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1223, Train Accuracy: 96.1400%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1223, Train Accuracy: 96.1400%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1160, Test Accuracy: 96.4200%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 9\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1160, Test Accuracy: 96.4200%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [6400/60000 (11%)], Train Loss: 0.541070;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [6400/60000 (11%)], Train Loss: 0.541070;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/60000 (21%)], Train Loss: 0.434201;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [12800/60000 (21%)], Train Loss: 0.434201;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [19200/60000 (32%)], Train Loss: 0.197400;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [19200/60000 (32%)], Train Loss: 0.197400;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/60000 (43%)], Train Loss: 0.469770;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [25600/60000 (43%)], Train Loss: 0.469770;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [32000/60000 (53%)], Train Loss: 0.159525;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [32000/60000 (53%)], Train Loss: 0.159525;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [38400/60000 (64%)], Train Loss: 0.757667;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [38400/60000 (64%)], Train Loss: 0.757667;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [44800/60000 (75%)], Train Loss: 0.284950;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [44800/60000 (75%)], Train Loss: 0.284950;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [51200/60000 (85%)], Train Loss: 0.334385;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [51200/60000 (85%)], Train Loss: 0.334385;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [57600/60000 (96%)], Train Loss: 0.203318;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [57600/60000 (96%)], Train Loss: 0.203318;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1043, Train Accuracy: 96.9667%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1043, Train Accuracy: 96.9667%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1033, Test Accuracy: 97.1400%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 10\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1033, Test Accuracy: 97.1400%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [6400/60000 (11%)], Train Loss: 0.277690;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [6400/60000 (11%)], Train Loss: 0.277690;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/60000 (21%)], Train Loss: 0.492950;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [12800/60000 (21%)], Train Loss: 0.492950;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19200/60000 (32%)], Train Loss: 0.284791;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [19200/60000 (32%)], Train Loss: 0.284791;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/60000 (43%)], Train Loss: 0.307791;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [25600/60000 (43%)], Train Loss: 0.307791;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [32000/60000 (53%)], Train Loss: 0.591849;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [32000/60000 (53%)], Train Loss: 0.591849;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [38400/60000 (64%)], Train Loss: 0.527022;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [38400/60000 (64%)], Train Loss: 0.527022;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [44800/60000 (75%)], Train Loss: 0.192943;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [44800/60000 (75%)], Train Loss: 0.192943;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [51200/60000 (85%)], Train Loss: 0.461220;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [51200/60000 (85%)], Train Loss: 0.461220;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [57600/60000 (96%)], Train Loss: 0.206218;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [57600/60000 (96%)], Train Loss: 0.206218;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1205, Train Accuracy: 96.3250%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1205, Train Accuracy: 96.3250%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1264, Test Accuracy: 96.4600%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1264, Test Accuracy: 96.4600%;\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34m2023-01-03 08:41:12,689 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-01-03 08:41:12,689 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-01-03 08:41:12,690 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-01-03 08:41:27 Uploading - Uploading generated training model\n",
      "2023-01-03 08:41:27 Completed - Training job completed\n",
      "Training seconds: 307\n",
      "Billable seconds: 307\n",
      "CPU times: user 1.41 s, sys: 126 ms, total: 1.54 s\n",
      "Wall time: 6min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Start training job with experiment setting\n",
    "run_name1=run_name+\"-1\"\n",
    "with Run(experiment_name=experiment_name, run_name=run_name1, sagemaker_session=Session()) as run:\n",
    "    estmator = PyTorch(\n",
    "        entry_point=\"./script/mnist.py\",\n",
    "        role=role,\n",
    "        model_dir=False,\n",
    "        framework_version=\"1.12\",\n",
    "        py_version=\"py38\",\n",
    "        instance_type=\"ml.c5.xlarge\",\n",
    "        instance_count=1,\n",
    "        hyperparameters={\"epochs\": 10, \"hidden_channels\": 5, \"optimizer\": \"adam\", \"region\": region},\n",
    "        keep_alive_period_in_seconds=3600,\n",
    "    )\n",
    "\n",
    "    estmator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf44ecb6-90d1-4d37-bfce-50e46de9f812",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.experiments.run:run_name is explicitly supplied in load_run, which will be prioritized to load the Run object. In other words, the run name in the experiment config, fetched from the job environment or the current run context, will be ignored.\n",
      "INFO:sagemaker.experiments.run:The run (experiment-run-example-1) under experiment (training-job-experiment-1672305742-bcbe) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-01-03-08-44-29-055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03 08:44:29 Starting - Starting the training job......\n",
      "2023-01-03 08:45:11 Starting - Preparing the instances for training......\n",
      "2023-01-03 08:46:23 Downloading - Downloading input data.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:04,491 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:04,493 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:04,494 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:04,503 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:04,505 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:04,669 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:04,671 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:04,681 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:04,683 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:04,693 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:04,695 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:04,704 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_channels\": 5,\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"region\": \"ap-southeast-2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-01-03-08-44-29-055\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-08-44-29-055/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"adam\",\"region\":\"ap-southeast-2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-08-44-29-055/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.c5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"adam\",\"region\":\"ap-southeast-2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-01-03-08-44-29-055\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-08-44-29-055/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_channels\",\"5\",\"--optimizer\",\"adam\",\"--region\",\"ap-southeast-2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_CHANNELS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=ap-southeast-2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region ap-southeast-2\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:05,214 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.122.0)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.126.0.tar.gz (654 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 654.9/654.9 kB 21.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.30 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2022.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.30->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sagemaker: filename=sagemaker-2.126.0-py2.py3-none-any.whl size=890069 sha256=379d30d1eaa043d57bfb87ff448c8cecef2cd9fdfc2f0a293e3cd4503809881d\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/8d/05/18/aed11de4cbba829d865b14cc3758e697e789af1ac01245e290\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.122.0\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.122.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.122.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-2.126.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 9912422/9912422 [00:00<00:00, 303304088.57it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/28881 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 28881/28881 [00:00<00:00, 17709896.76it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1648877/1648877 [00:00<00:00, 74183352.43it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/4542 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 4542/4542 [00:00<00:00, 55866653.28it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"mnist.py\", line 246, in <module>\u001b[0m\n",
      "\u001b[34mwith load_run(sagemaker_session=session) as run:\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/sagemaker/experiments/run.py\", line 798, in load_run\u001b[0m\n",
      "\u001b[34mexp_config = get_tc_and_exp_config_from_job_env(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/sagemaker/experiments/_utils.py\", line 142, in get_tc_and_exp_config_from_job_env\u001b[0m\n",
      "\u001b[34mraise RuntimeError(\u001b[0m\n",
      "\u001b[34mRuntimeError: Not able to fetch RunName in ExperimentConfig of the sagemaker job. Please make sure the ExperimentConfig is correctly set.\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:11,041 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:11,041 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:11,041 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:11,041 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"raise RuntimeError(\n",
      " RuntimeError: Not able to fetch RunName in ExperimentConfig of the sagemaker job. Please make sure the ExperimentConfig is correctly set.\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region ap-southeast-2\"\u001b[0m\n",
      "\u001b[34m2023-01-03 08:47:11,041 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2023-01-03 08:47:25 Training - Training image download completed. Training in progress.\n",
      "2023-01-03 08:47:25 Uploading - Uploading generated training model\n",
      "2023-01-03 08:47:25 Failed - Instances not retained as a result of warmpool resource limits being exceeded\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the UpdateTrialComponent operation: 1 validation error detected: Value 'Error for Training job pytorch-training-2023-01-03-08-44-29-055: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise RuntimeError(\n RuntimeError: Not able to fetch RunName in ExperimentConfig of the sagemaker job. Please make sure the ExperimentConfig is correctly set.\"\nCommand \"/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region ap-southeast-2\", exit code: 1' at 'status.message' failed to satisfy constraint: Member must satisfy regular expression pattern: .*",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   4112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4113\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3643\u001b[0m                 )\n\u001b[0;32m-> 3644\u001b[0;31m             raise exceptions.UnexpectedStatusException(\n\u001b[0m\u001b[1;32m   3645\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2023-01-03-08-44-29-055: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise RuntimeError(\n RuntimeError: Not able to fetch RunName in ExperimentConfig of the sagemaker job. Please make sure the ExperimentConfig is correctly set.\"\nCommand \"/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region ap-southeast-2\", exit code: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/experiments/run.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, exc_traceback)\u001b[0m\n\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/experiments/run.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0;31m# Update the trial component with additions from the Run object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_component\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m             \u001b[0;31m# Create Lineage entities for the artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lineage_artifact_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/experiments/trial_component.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;34m\"\"\"Save the state of this TrialComponent to SageMaker.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boto_update_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boto_update_members\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_disassociate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/apiutils/_base_types.py\u001b[0m in \u001b[0;36m_invoke_api\u001b[0;34m(self, boto_method, boto_method_members)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mapi_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_boto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mapi_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboto_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mapi_boto_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mapi_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_boto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_boto_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 )\n\u001b[1;32m    529\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the UpdateTrialComponent operation: 1 validation error detected: Value 'Error for Training job pytorch-training-2023-01-03-08-44-29-055: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise RuntimeError(\n RuntimeError: Not able to fetch RunName in ExperimentConfig of the sagemaker job. Please make sure the ExperimentConfig is correctly set.\"\nCommand \"/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region ap-southeast-2\", exit code: 1' at 'status.message' failed to satisfy constraint: Member must satisfy regular expression pattern: .*"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Start training job with experiment setting\n",
    "run_name1=run_name+\"-1\"\n",
    "with load_run(experiment_name=experiment_name, run_name=run_name1, sagemaker_session=Session()) as run:\n",
    "    estmator = PyTorch(\n",
    "        entry_point=\"./script/mnist.py\",\n",
    "        role=role,\n",
    "        model_dir=False,\n",
    "        framework_version=\"1.12\",\n",
    "        py_version=\"py38\",\n",
    "        instance_type=\"ml.c5.xlarge\",\n",
    "        instance_count=1,\n",
    "        hyperparameters={\"epochs\": 10, \"hidden_channels\": 5, \"optimizer\": \"adam\", \"region\": region},\n",
    "        keep_alive_period_in_seconds=3600,\n",
    "    )\n",
    "\n",
    "    estmator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46699a60-5af5-4b51-959c-06e6049dc29f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.experiments.run:The run (experiment-run-example-2) under experiment (training-job-experiment-1672305742-bcbe) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_channels- 5  optimizer- adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-01-03-09-18-08-421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03 09:18:09 Starting - Starting the training job...\n",
      "2023-01-03 09:18:23 Starting - Preparing the instances for training......\n",
      "2023-01-03 09:19:33 Downloading - Downloading input data.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:14,722 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:14,724 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:14,726 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:14,735 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:14,737 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:14,931 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:14,933 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:14,949 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:14,951 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:14,962 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:14,963 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:14,973 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_channels\": 5,\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"region\": \"ap-southeast-2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-01-03-09-18-08-421\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-09-18-08-421/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"adam\",\"region\":\"ap-southeast-2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-09-18-08-421/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.c5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"adam\",\"region\":\"ap-southeast-2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-01-03-09-18-08-421\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-09-18-08-421/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_channels\",\"5\",\"--optimizer\",\"adam\",\"--region\",\"ap-southeast-2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_CHANNELS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=ap-southeast-2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region ap-southeast-2\u001b[0m\n",
      "\u001b[34m2023-01-03 09:20:15,529 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.122.0)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.126.0.tar.gz (654 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 654.9/654.9 kB 14.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.30 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2022.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.30->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sagemaker: filename=sagemaker-2.126.0-py2.py3-none-any.whl size=890069 sha256=479f8a300d4153c99ac04ce60c7389811f3934859f39e4aa7c9b8e471edf5f30\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/8d/05/18/aed11de4cbba829d865b14cc3758e697e789af1ac01245e290\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.122.0\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.122.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.122.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-2.126.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 9912422/9912422 [00:00<00:00, 104325019.87it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/28881 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 28881/28881 [00:00<00:00, 28959047.05it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1648877/1648877 [00:00<00:00, 178465405.57it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/4542 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 4542/4542 [00:00<00:00, 39771458.81it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.experiments.run:The run (experiment-run-example-2) under experiment (training-job-experiment-1672305742-bcbe) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mTraining Epoch: 1\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.635 algo-1:43 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.780 algo-1:43 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.781 algo-1:43 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.782 algo-1:43 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.782 algo-1:43 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.782 algo-1:43 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.796 algo-1:43 INFO hook.py:561] name:module.conv1.weight count_params:125\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.796 algo-1:43 INFO hook.py:561] name:module.conv1.bias count_params:5\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.796 algo-1:43 INFO hook.py:561] name:module.conv2.weight count_params:2500\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.796 algo-1:43 INFO hook.py:561] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.796 algo-1:43 INFO hook.py:561] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.796 algo-1:43 INFO hook.py:561] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.796 algo-1:43 INFO hook.py:561] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.796 algo-1:43 INFO hook.py:561] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.796 algo-1:43 INFO hook.py:563] Total Trainable Params: 19210\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.797 algo-1:43 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:20:23.798 algo-1:43 INFO hook.py:485] Hook is writing from the hook with pid: 43\u001b[0m\n",
      "\n",
      "2023-01-03 09:20:14 Training - Training image download completed. Training in progress.\u001b[34mTrain Epoch: 1 [6400/60000 (11%)], Train Loss: 0.619187;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)], Train Loss: 0.619187;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)], Train Loss: 0.841508;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)], Train Loss: 0.841508;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)], Train Loss: 0.535673;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)], Train Loss: 0.535673;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)], Train Loss: 0.633673;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)], Train Loss: 0.633673;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)], Train Loss: 0.604052;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)], Train Loss: 0.604052;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)], Train Loss: 0.196605;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)], Train Loss: 0.196605;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)], Train Loss: 0.764071;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)], Train Loss: 0.764071;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)], Train Loss: 0.366877;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)], Train Loss: 0.366877;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)], Train Loss: 0.381978;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)], Train Loss: 0.381978;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1360, Train Accuracy: 95.8583%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1360, Train Accuracy: 95.8583%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1266, Test Accuracy: 96.3100%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1266, Test Accuracy: 96.3100%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)], Train Loss: 0.249367;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)], Train Loss: 0.249367;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)], Train Loss: 0.260965;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)], Train Loss: 0.260965;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)], Train Loss: 0.341665;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)], Train Loss: 0.341665;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)], Train Loss: 0.254568;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)], Train Loss: 0.254568;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)], Train Loss: 0.625225;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)], Train Loss: 0.625225;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)], Train Loss: 0.271208;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)], Train Loss: 0.271208;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)], Train Loss: 0.241313;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)], Train Loss: 0.241313;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)], Train Loss: 0.258134;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)], Train Loss: 0.258134;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)], Train Loss: 0.276261;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)], Train Loss: 0.276261;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1211, Train Accuracy: 96.2967%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1211, Train Accuracy: 96.2967%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1162, Test Accuracy: 96.4100%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 3\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1162, Test Accuracy: 96.4100%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/60000 (11%)], Train Loss: 0.540804;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)], Train Loss: 0.540804;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)], Train Loss: 0.507104;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)], Train Loss: 0.507104;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/60000 (32%)], Train Loss: 0.732960;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)], Train Loss: 0.732960;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)], Train Loss: 0.478505;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)], Train Loss: 0.478505;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32000/60000 (53%)], Train Loss: 0.201122;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)], Train Loss: 0.201122;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)], Train Loss: 0.261213;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)], Train Loss: 0.261213;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/60000 (75%)], Train Loss: 0.312363;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)], Train Loss: 0.312363;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)], Train Loss: 0.468251;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)], Train Loss: 0.468251;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [57600/60000 (96%)], Train Loss: 0.477161;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)], Train Loss: 0.477161;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1195, Train Accuracy: 96.5067%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1195, Train Accuracy: 96.5067%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1105, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 4\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1105, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/60000 (11%)], Train Loss: 0.333059;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)], Train Loss: 0.333059;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)], Train Loss: 0.165329;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)], Train Loss: 0.165329;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/60000 (32%)], Train Loss: 0.313524;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)], Train Loss: 0.313524;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)], Train Loss: 0.170249;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)], Train Loss: 0.170249;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32000/60000 (53%)], Train Loss: 0.286480;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)], Train Loss: 0.286480;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)], Train Loss: 0.191275;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)], Train Loss: 0.191275;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44800/60000 (75%)], Train Loss: 0.423735;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)], Train Loss: 0.423735;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)], Train Loss: 0.274153;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)], Train Loss: 0.274153;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [57600/60000 (96%)], Train Loss: 0.593129;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)], Train Loss: 0.593129;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1136, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1136, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1061, Test Accuracy: 96.7700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 5\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1061, Test Accuracy: 96.7700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/60000 (11%)], Train Loss: 0.343393;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)], Train Loss: 0.343393;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)], Train Loss: 0.390378;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)], Train Loss: 0.390378;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/60000 (32%)], Train Loss: 0.360940;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)], Train Loss: 0.360940;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)], Train Loss: 0.489606;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)], Train Loss: 0.489606;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32000/60000 (53%)], Train Loss: 0.268219;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)], Train Loss: 0.268219;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)], Train Loss: 0.237594;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)], Train Loss: 0.237594;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44800/60000 (75%)], Train Loss: 0.415231;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)], Train Loss: 0.415231;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)], Train Loss: 0.176550;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)], Train Loss: 0.176550;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [57600/60000 (96%)], Train Loss: 0.283533;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)], Train Loss: 0.283533;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1177, Train Accuracy: 96.5850%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1177, Train Accuracy: 96.5850%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1108, Test Accuracy: 96.8700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 6\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1108, Test Accuracy: 96.8700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/60000 (11%)], Train Loss: 0.270528;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [6400/60000 (11%)], Train Loss: 0.270528;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/60000 (21%)], Train Loss: 0.321458;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [12800/60000 (21%)], Train Loss: 0.321458;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/60000 (32%)], Train Loss: 0.289076;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [19200/60000 (32%)], Train Loss: 0.289076;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/60000 (43%)], Train Loss: 0.365472;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [25600/60000 (43%)], Train Loss: 0.365472;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [32000/60000 (53%)], Train Loss: 0.418724;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [32000/60000 (53%)], Train Loss: 0.418724;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [38400/60000 (64%)], Train Loss: 0.341385;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [38400/60000 (64%)], Train Loss: 0.341385;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [44800/60000 (75%)], Train Loss: 0.121961;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [44800/60000 (75%)], Train Loss: 0.121961;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/60000 (85%)], Train Loss: 0.361106;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [51200/60000 (85%)], Train Loss: 0.361106;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [57600/60000 (96%)], Train Loss: 0.265346;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [57600/60000 (96%)], Train Loss: 0.265346;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1122, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1122, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1090, Test Accuracy: 96.6700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 7\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1090, Test Accuracy: 96.6700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [6400/60000 (11%)], Train Loss: 0.264878;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [6400/60000 (11%)], Train Loss: 0.264878;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/60000 (21%)], Train Loss: 0.377314;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [12800/60000 (21%)], Train Loss: 0.377314;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19200/60000 (32%)], Train Loss: 0.282708;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [19200/60000 (32%)], Train Loss: 0.282708;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/60000 (43%)], Train Loss: 0.270151;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [25600/60000 (43%)], Train Loss: 0.270151;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [32000/60000 (53%)], Train Loss: 0.463462;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [32000/60000 (53%)], Train Loss: 0.463462;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [38400/60000 (64%)], Train Loss: 0.318820;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [38400/60000 (64%)], Train Loss: 0.318820;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [44800/60000 (75%)], Train Loss: 0.301752;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [44800/60000 (75%)], Train Loss: 0.301752;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [51200/60000 (85%)], Train Loss: 0.491330;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [51200/60000 (85%)], Train Loss: 0.491330;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [57600/60000 (96%)], Train Loss: 0.605309;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [57600/60000 (96%)], Train Loss: 0.605309;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1232, Train Accuracy: 96.2983%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1232, Train Accuracy: 96.2983%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1162, Test Accuracy: 96.6500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 8\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1162, Test Accuracy: 96.6500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [6400/60000 (11%)], Train Loss: 0.436534;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [6400/60000 (11%)], Train Loss: 0.436534;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/60000 (21%)], Train Loss: 0.697182;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [12800/60000 (21%)], Train Loss: 0.697182;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [19200/60000 (32%)], Train Loss: 0.352988;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [19200/60000 (32%)], Train Loss: 0.352988;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/60000 (43%)], Train Loss: 0.251108;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [25600/60000 (43%)], Train Loss: 0.251108;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [32000/60000 (53%)], Train Loss: 0.337925;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [32000/60000 (53%)], Train Loss: 0.337925;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [38400/60000 (64%)], Train Loss: 0.243655;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [38400/60000 (64%)], Train Loss: 0.243655;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [44800/60000 (75%)], Train Loss: 0.260882;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [44800/60000 (75%)], Train Loss: 0.260882;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [51200/60000 (85%)], Train Loss: 0.439534;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [51200/60000 (85%)], Train Loss: 0.439534;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [57600/60000 (96%)], Train Loss: 0.307441;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [57600/60000 (96%)], Train Loss: 0.307441;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1223, Train Accuracy: 96.1400%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1223, Train Accuracy: 96.1400%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1160, Test Accuracy: 96.4200%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 9\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1160, Test Accuracy: 96.4200%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [6400/60000 (11%)], Train Loss: 0.541070;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [6400/60000 (11%)], Train Loss: 0.541070;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/60000 (21%)], Train Loss: 0.434201;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [12800/60000 (21%)], Train Loss: 0.434201;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [19200/60000 (32%)], Train Loss: 0.197400;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [19200/60000 (32%)], Train Loss: 0.197400;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/60000 (43%)], Train Loss: 0.469770;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [25600/60000 (43%)], Train Loss: 0.469770;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [32000/60000 (53%)], Train Loss: 0.159525;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [32000/60000 (53%)], Train Loss: 0.159525;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [38400/60000 (64%)], Train Loss: 0.757667;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [38400/60000 (64%)], Train Loss: 0.757667;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [44800/60000 (75%)], Train Loss: 0.284950;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [44800/60000 (75%)], Train Loss: 0.284950;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [51200/60000 (85%)], Train Loss: 0.334385;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [51200/60000 (85%)], Train Loss: 0.334385;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [57600/60000 (96%)], Train Loss: 0.203318;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [57600/60000 (96%)], Train Loss: 0.203318;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1043, Train Accuracy: 96.9667%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1043, Train Accuracy: 96.9667%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1033, Test Accuracy: 97.1400%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 10\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1033, Test Accuracy: 97.1400%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [6400/60000 (11%)], Train Loss: 0.277690;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [6400/60000 (11%)], Train Loss: 0.277690;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/60000 (21%)], Train Loss: 0.492950;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [12800/60000 (21%)], Train Loss: 0.492950;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19200/60000 (32%)], Train Loss: 0.284791;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [19200/60000 (32%)], Train Loss: 0.284791;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/60000 (43%)], Train Loss: 0.307791;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [25600/60000 (43%)], Train Loss: 0.307791;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [32000/60000 (53%)], Train Loss: 0.591849;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [32000/60000 (53%)], Train Loss: 0.591849;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [38400/60000 (64%)], Train Loss: 0.527022;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [38400/60000 (64%)], Train Loss: 0.527022;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [44800/60000 (75%)], Train Loss: 0.192943;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [44800/60000 (75%)], Train Loss: 0.192943;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [51200/60000 (85%)], Train Loss: 0.461220;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [51200/60000 (85%)], Train Loss: 0.461220;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [57600/60000 (96%)], Train Loss: 0.206218;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [57600/60000 (96%)], Train Loss: 0.206218;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1205, Train Accuracy: 96.3250%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1205, Train Accuracy: 96.3250%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1264, Test Accuracy: 96.4600%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1264, Test Accuracy: 96.4600%;\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\n",
      "2023-01-03 09:24:40 Uploading - Uploading generated training model\u001b[34m2023-01-03 09:24:35,990 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:24:35,990 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:24:35,990 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-01-03 09:24:51 Completed - Training job completed\n",
      "Training seconds: 319\n",
      "Billable seconds: 319\n",
      "hidden_channels- 5  optimizer- sgd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-01-03-09-25-25-851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03 09:25:26 Starting - Starting the training job...\n",
      "2023-01-03 09:25:41 Starting - Preparing the instances for training.........\n",
      "2023-01-03 09:27:18 Downloading - Downloading input data\n",
      "2023-01-03 09:27:18 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,156 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,157 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,159 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,168 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,170 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,350 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,352 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,363 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,365 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,380 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,382 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,394 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_channels\": 5,\n",
      "        \"optimizer\": \"sgd\",\n",
      "        \"region\": \"ap-southeast-2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-01-03-09-25-25-851\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-09-25-25-851/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"sgd\",\"region\":\"ap-southeast-2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-09-25-25-851/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.c5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"sgd\",\"region\":\"ap-southeast-2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-01-03-09-25-25-851\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-09-25-25-851/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_channels\",\"5\",\"--optimizer\",\"sgd\",\"--region\",\"ap-southeast-2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_CHANNELS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=sgd\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=ap-southeast-2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer sgd --region ap-southeast-2\u001b[0m\n",
      "\u001b[34m2023-01-03 09:27:19,948 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.122.0)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.126.0.tar.gz (654 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 654.9/654.9 kB 11.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.30 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2022.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.30->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sagemaker: filename=sagemaker-2.126.0-py2.py3-none-any.whl size=890069 sha256=4ddd38859edab7391191b6892a70daef45c13ea5ec9e4ac36c51e6471aaaccfb\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/8d/05/18/aed11de4cbba829d865b14cc3758e697e789af1ac01245e290\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.122.0\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.122.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.122.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-2.126.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 9912422/9912422 [00:00<00:00, 300967940.09it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/28881 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 28881/28881 [00:00<00:00, 43873847.82it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1648877/1648877 [00:00<00:00, 172401630.23it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/4542 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 4542/4542 [00:00<00:00, 39360596.63it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.experiments.run:The run (experiment-run-example-3) under experiment (training-job-experiment-1672305742-bcbe) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mTraining Epoch: 1\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:27.989 algo-1:43 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.177 algo-1:43 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.179 algo-1:43 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.179 algo-1:43 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.180 algo-1:43 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.180 algo-1:43 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.199 algo-1:43 INFO hook.py:561] name:module.conv1.weight count_params:125\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.199 algo-1:43 INFO hook.py:561] name:module.conv1.bias count_params:5\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.199 algo-1:43 INFO hook.py:561] name:module.conv2.weight count_params:2500\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.199 algo-1:43 INFO hook.py:561] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.199 algo-1:43 INFO hook.py:561] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.199 algo-1:43 INFO hook.py:561] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.199 algo-1:43 INFO hook.py:561] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.199 algo-1:43 INFO hook.py:561] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.199 algo-1:43 INFO hook.py:563] Total Trainable Params: 19210\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.199 algo-1:43 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:27:28.202 algo-1:43 INFO hook.py:485] Hook is writing from the hook with pid: 43\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/60000 (11%)], Train Loss: 2.161843;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)], Train Loss: 2.161843;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)], Train Loss: 1.420992;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)], Train Loss: 1.420992;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)], Train Loss: 0.819197;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)], Train Loss: 0.819197;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)], Train Loss: 1.024498;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)], Train Loss: 1.024498;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)], Train Loss: 0.831653;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)], Train Loss: 0.831653;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)], Train Loss: 0.622700;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)], Train Loss: 0.622700;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)], Train Loss: 0.663551;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)], Train Loss: 0.663551;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)], Train Loss: 0.823202;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)], Train Loss: 0.823202;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)], Train Loss: 0.648185;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)], Train Loss: 0.648185;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.2266, Train Accuracy: 93.1933%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.2266, Train Accuracy: 93.1933%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.2137, Test Accuracy: 93.4000%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.2137, Test Accuracy: 93.4000%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)], Train Loss: 0.369348;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)], Train Loss: 0.369348;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)], Train Loss: 0.360285;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)], Train Loss: 0.360285;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)], Train Loss: 0.352757;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)], Train Loss: 0.352757;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)], Train Loss: 0.521665;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)], Train Loss: 0.521665;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)], Train Loss: 0.620596;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)], Train Loss: 0.620596;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)], Train Loss: 0.354078;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)], Train Loss: 0.354078;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)], Train Loss: 0.292021;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)], Train Loss: 0.292021;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)], Train Loss: 0.477928;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)], Train Loss: 0.477928;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)], Train Loss: 0.304993;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)], Train Loss: 0.304993;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1385, Train Accuracy: 95.7700%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1385, Train Accuracy: 95.7700%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1277, Test Accuracy: 95.9700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 3\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1277, Test Accuracy: 95.9700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/60000 (11%)], Train Loss: 0.262212;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)], Train Loss: 0.262212;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)], Train Loss: 0.267716;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)], Train Loss: 0.267716;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/60000 (32%)], Train Loss: 0.266506;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)], Train Loss: 0.266506;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)], Train Loss: 0.514212;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)], Train Loss: 0.514212;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32000/60000 (53%)], Train Loss: 0.289451;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)], Train Loss: 0.289451;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)], Train Loss: 0.363232;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)], Train Loss: 0.363232;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/60000 (75%)], Train Loss: 0.238085;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)], Train Loss: 0.238085;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)], Train Loss: 0.349704;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)], Train Loss: 0.349704;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [57600/60000 (96%)], Train Loss: 0.339514;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)], Train Loss: 0.339514;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1109, Train Accuracy: 96.6800%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1109, Train Accuracy: 96.6800%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1018, Test Accuracy: 96.9400%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 4\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1018, Test Accuracy: 96.9400%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/60000 (11%)], Train Loss: 0.257811;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)], Train Loss: 0.257811;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)], Train Loss: 0.270994;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)], Train Loss: 0.270994;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/60000 (32%)], Train Loss: 0.459039;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)], Train Loss: 0.459039;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)], Train Loss: 0.235574;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)], Train Loss: 0.235574;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32000/60000 (53%)], Train Loss: 0.330875;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)], Train Loss: 0.330875;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)], Train Loss: 0.213651;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)], Train Loss: 0.213651;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44800/60000 (75%)], Train Loss: 0.225432;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)], Train Loss: 0.225432;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)], Train Loss: 0.116112;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)], Train Loss: 0.116112;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [57600/60000 (96%)], Train Loss: 0.359282;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)], Train Loss: 0.359282;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0938, Train Accuracy: 97.1550%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0938, Train Accuracy: 97.1550%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0838, Test Accuracy: 97.4800%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 5\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0838, Test Accuracy: 97.4800%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/60000 (11%)], Train Loss: 0.311842;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)], Train Loss: 0.311842;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)], Train Loss: 0.176747;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)], Train Loss: 0.176747;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/60000 (32%)], Train Loss: 0.218995;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)], Train Loss: 0.218995;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)], Train Loss: 0.254856;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)], Train Loss: 0.254856;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32000/60000 (53%)], Train Loss: 0.205307;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)], Train Loss: 0.205307;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)], Train Loss: 0.257434;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)], Train Loss: 0.257434;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44800/60000 (75%)], Train Loss: 0.307852;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)], Train Loss: 0.307852;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)], Train Loss: 0.070743;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)], Train Loss: 0.070743;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [57600/60000 (96%)], Train Loss: 0.340530;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)], Train Loss: 0.340530;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0894, Train Accuracy: 97.2433%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0894, Train Accuracy: 97.2433%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0838, Test Accuracy: 97.2700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 6\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0838, Test Accuracy: 97.2700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/60000 (11%)], Train Loss: 0.262699;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [6400/60000 (11%)], Train Loss: 0.262699;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/60000 (21%)], Train Loss: 0.205534;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [12800/60000 (21%)], Train Loss: 0.205534;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/60000 (32%)], Train Loss: 0.190471;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [19200/60000 (32%)], Train Loss: 0.190471;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/60000 (43%)], Train Loss: 0.308710;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [25600/60000 (43%)], Train Loss: 0.308710;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [32000/60000 (53%)], Train Loss: 0.195560;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [32000/60000 (53%)], Train Loss: 0.195560;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [38400/60000 (64%)], Train Loss: 0.213704;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [38400/60000 (64%)], Train Loss: 0.213704;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [44800/60000 (75%)], Train Loss: 0.163944;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [44800/60000 (75%)], Train Loss: 0.163944;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/60000 (85%)], Train Loss: 0.145806;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [51200/60000 (85%)], Train Loss: 0.145806;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [57600/60000 (96%)], Train Loss: 0.157533;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [57600/60000 (96%)], Train Loss: 0.157533;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0772, Train Accuracy: 97.6167%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0772, Train Accuracy: 97.6167%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0689, Test Accuracy: 97.7900%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 7\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0689, Test Accuracy: 97.7900%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [6400/60000 (11%)], Train Loss: 0.083704;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [6400/60000 (11%)], Train Loss: 0.083704;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/60000 (21%)], Train Loss: 0.326774;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [12800/60000 (21%)], Train Loss: 0.326774;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19200/60000 (32%)], Train Loss: 0.251189;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [19200/60000 (32%)], Train Loss: 0.251189;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/60000 (43%)], Train Loss: 0.148258;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [25600/60000 (43%)], Train Loss: 0.148258;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [32000/60000 (53%)], Train Loss: 0.388826;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [32000/60000 (53%)], Train Loss: 0.388826;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [38400/60000 (64%)], Train Loss: 0.124279;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [38400/60000 (64%)], Train Loss: 0.124279;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [44800/60000 (75%)], Train Loss: 0.298224;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [44800/60000 (75%)], Train Loss: 0.298224;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [51200/60000 (85%)], Train Loss: 0.264435;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [51200/60000 (85%)], Train Loss: 0.264435;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [57600/60000 (96%)], Train Loss: 0.334551;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [57600/60000 (96%)], Train Loss: 0.334551;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0726, Train Accuracy: 97.8167%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0726, Train Accuracy: 97.8167%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0674, Test Accuracy: 98.0500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 8\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0674, Test Accuracy: 98.0500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [6400/60000 (11%)], Train Loss: 0.231937;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [6400/60000 (11%)], Train Loss: 0.231937;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/60000 (21%)], Train Loss: 0.242870;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [12800/60000 (21%)], Train Loss: 0.242870;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [19200/60000 (32%)], Train Loss: 0.279829;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [19200/60000 (32%)], Train Loss: 0.279829;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/60000 (43%)], Train Loss: 0.135758;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [25600/60000 (43%)], Train Loss: 0.135758;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [32000/60000 (53%)], Train Loss: 0.362022;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [32000/60000 (53%)], Train Loss: 0.362022;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [38400/60000 (64%)], Train Loss: 0.171376;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [38400/60000 (64%)], Train Loss: 0.171376;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [44800/60000 (75%)], Train Loss: 0.310994;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [44800/60000 (75%)], Train Loss: 0.310994;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [51200/60000 (85%)], Train Loss: 0.220572;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [51200/60000 (85%)], Train Loss: 0.220572;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [57600/60000 (96%)], Train Loss: 0.141152;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [57600/60000 (96%)], Train Loss: 0.141152;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0674, Train Accuracy: 97.9933%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0674, Train Accuracy: 97.9933%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0616, Test Accuracy: 98.1300%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 9\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0616, Test Accuracy: 98.1300%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [6400/60000 (11%)], Train Loss: 0.271839;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [6400/60000 (11%)], Train Loss: 0.271839;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/60000 (21%)], Train Loss: 0.298069;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [12800/60000 (21%)], Train Loss: 0.298069;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [19200/60000 (32%)], Train Loss: 0.050917;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [19200/60000 (32%)], Train Loss: 0.050917;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/60000 (43%)], Train Loss: 0.183261;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [25600/60000 (43%)], Train Loss: 0.183261;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [32000/60000 (53%)], Train Loss: 0.102819;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [32000/60000 (53%)], Train Loss: 0.102819;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [38400/60000 (64%)], Train Loss: 0.169136;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [38400/60000 (64%)], Train Loss: 0.169136;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [44800/60000 (75%)], Train Loss: 0.175098;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [44800/60000 (75%)], Train Loss: 0.175098;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [51200/60000 (85%)], Train Loss: 0.252922;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [51200/60000 (85%)], Train Loss: 0.252922;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [57600/60000 (96%)], Train Loss: 0.123683;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [57600/60000 (96%)], Train Loss: 0.123683;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0642, Train Accuracy: 98.0500%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0642, Train Accuracy: 98.0500%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0610, Test Accuracy: 98.1200%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 10\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0610, Test Accuracy: 98.1200%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [6400/60000 (11%)], Train Loss: 0.204782;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [6400/60000 (11%)], Train Loss: 0.204782;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/60000 (21%)], Train Loss: 0.305953;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [12800/60000 (21%)], Train Loss: 0.305953;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19200/60000 (32%)], Train Loss: 0.096279;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [19200/60000 (32%)], Train Loss: 0.096279;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/60000 (43%)], Train Loss: 0.187483;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [25600/60000 (43%)], Train Loss: 0.187483;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [32000/60000 (53%)], Train Loss: 0.310414;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [32000/60000 (53%)], Train Loss: 0.310414;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [38400/60000 (64%)], Train Loss: 0.191778;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [38400/60000 (64%)], Train Loss: 0.191778;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [44800/60000 (75%)], Train Loss: 0.128569;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [44800/60000 (75%)], Train Loss: 0.128569;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [51200/60000 (85%)], Train Loss: 0.143577;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [51200/60000 (85%)], Train Loss: 0.143577;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [57600/60000 (96%)], Train Loss: 0.036914;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [57600/60000 (96%)], Train Loss: 0.036914;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0603, Train Accuracy: 98.1783%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0603, Train Accuracy: 98.1783%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0536, Test Accuracy: 98.2900%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0536, Test Accuracy: 98.2900%;\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:31:34,273 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:31:34,273 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:31:34,274 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-01-03 09:31:50 Uploading - Uploading generated training model\n",
      "2023-01-03 09:31:50 Completed - Training job completed\n",
      "Training seconds: 293\n",
      "Billable seconds: 293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_channels- 10  optimizer- adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-01-03-09-32-13-303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03 09:32:13 Starting - Starting the training job...\n",
      "2023-01-03 09:32:31 Starting - Preparing the instances for training.........\n",
      "2023-01-03 09:34:11 Downloading - Downloading input data\n",
      "2023-01-03 09:34:11 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:12,364 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:12,365 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:12,367 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:12,376 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:12,378 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:12,564 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:12,566 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:12,577 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:12,578 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:12,589 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:12,591 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:12,600 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_channels\": 10,\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"region\": \"ap-southeast-2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-01-03-09-32-13-303\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-09-32-13-303/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_channels\":10,\"optimizer\":\"adam\",\"region\":\"ap-southeast-2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-09-32-13-303/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.c5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_channels\":10,\"optimizer\":\"adam\",\"region\":\"ap-southeast-2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-01-03-09-32-13-303\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-09-32-13-303/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_channels\",\"10\",\"--optimizer\",\"adam\",\"--region\",\"ap-southeast-2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_CHANNELS=10\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=ap-southeast-2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 10 --optimizer adam --region ap-southeast-2\u001b[0m\n",
      "\u001b[34m2023-01-03 09:34:13,145 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.122.0)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.126.0.tar.gz (654 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 654.9/654.9 kB 18.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.30 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2022.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.30->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sagemaker: filename=sagemaker-2.126.0-py2.py3-none-any.whl size=890069 sha256=27be008e0358e2a50219cc1a9e13cc0edbd757ffeef2bb6f3b195dfe88fdb3e5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/8d/05/18/aed11de4cbba829d865b14cc3758e697e789af1ac01245e290\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.122.0\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.122.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.122.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-2.126.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 9912422/9912422 [00:00<00:00, 298455246.80it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/28881 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 28881/28881 [00:00<00:00, 23906787.81it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1648877/1648877 [00:00<00:00, 123321886.53it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/4542 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 4542/4542 [00:00<00:00, 37723819.34it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.experiments.run:The run (experiment-run-example-4) under experiment (training-job-experiment-1672305742-bcbe) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mTraining Epoch: 1\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:20.903 algo-1:43 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.049 algo-1:43 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.050 algo-1:43 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.051 algo-1:43 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.051 algo-1:43 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.051 algo-1:43 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.063 algo-1:43 INFO hook.py:561] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.063 algo-1:43 INFO hook.py:561] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.063 algo-1:43 INFO hook.py:561] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.063 algo-1:43 INFO hook.py:561] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.064 algo-1:43 INFO hook.py:561] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.064 algo-1:43 INFO hook.py:561] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.064 algo-1:43 INFO hook.py:561] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.064 algo-1:43 INFO hook.py:561] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.064 algo-1:43 INFO hook.py:563] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.064 algo-1:43 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:34:21.066 algo-1:43 INFO hook.py:485] Hook is writing from the hook with pid: 43\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/60000 (11%)], Train Loss: 0.752171;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)], Train Loss: 0.752171;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)], Train Loss: 0.732862;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)], Train Loss: 0.732862;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)], Train Loss: 0.452901;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)], Train Loss: 0.452901;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)], Train Loss: 0.601330;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)], Train Loss: 0.601330;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)], Train Loss: 0.663335;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)], Train Loss: 0.663335;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)], Train Loss: 0.434520;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)], Train Loss: 0.434520;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)], Train Loss: 0.560445;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)], Train Loss: 0.560445;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)], Train Loss: 0.282431;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)], Train Loss: 0.282431;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)], Train Loss: 0.339657;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)], Train Loss: 0.339657;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1475, Train Accuracy: 95.6067%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1475, Train Accuracy: 95.6067%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1319, Test Accuracy: 96.4000%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1319, Test Accuracy: 96.4000%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)], Train Loss: 0.704372;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)], Train Loss: 0.704372;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)], Train Loss: 0.655197;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)], Train Loss: 0.655197;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)], Train Loss: 0.259430;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)], Train Loss: 0.259430;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)], Train Loss: 0.162441;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)], Train Loss: 0.162441;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)], Train Loss: 0.357910;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)], Train Loss: 0.357910;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)], Train Loss: 0.436255;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)], Train Loss: 0.436255;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)], Train Loss: 0.288760;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)], Train Loss: 0.288760;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)], Train Loss: 0.331606;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)], Train Loss: 0.331606;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)], Train Loss: 0.617510;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)], Train Loss: 0.617510;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1166, Train Accuracy: 96.5433%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1166, Train Accuracy: 96.5433%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1099, Test Accuracy: 96.8400%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 3\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1099, Test Accuracy: 96.8400%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/60000 (11%)], Train Loss: 0.481257;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)], Train Loss: 0.481257;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)], Train Loss: 0.382897;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)], Train Loss: 0.382897;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/60000 (32%)], Train Loss: 0.259243;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)], Train Loss: 0.259243;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)], Train Loss: 0.104538;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)], Train Loss: 0.104538;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32000/60000 (53%)], Train Loss: 0.506446;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)], Train Loss: 0.506446;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)], Train Loss: 0.310610;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)], Train Loss: 0.310610;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/60000 (75%)], Train Loss: 0.284107;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)], Train Loss: 0.284107;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)], Train Loss: 0.728084;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)], Train Loss: 0.728084;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [57600/60000 (96%)], Train Loss: 0.564850;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)], Train Loss: 0.564850;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1661, Train Accuracy: 94.8450%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1661, Train Accuracy: 94.8450%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1588, Test Accuracy: 95.1800%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 4\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1588, Test Accuracy: 95.1800%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/60000 (11%)], Train Loss: 0.270601;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)], Train Loss: 0.270601;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)], Train Loss: 0.616496;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)], Train Loss: 0.616496;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/60000 (32%)], Train Loss: 0.464752;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)], Train Loss: 0.464752;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)], Train Loss: 0.368399;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)], Train Loss: 0.368399;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32000/60000 (53%)], Train Loss: 0.347245;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)], Train Loss: 0.347245;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)], Train Loss: 0.198950;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)], Train Loss: 0.198950;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44800/60000 (75%)], Train Loss: 0.407197;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)], Train Loss: 0.407197;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)], Train Loss: 0.355184;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)], Train Loss: 0.355184;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [57600/60000 (96%)], Train Loss: 0.310600;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)], Train Loss: 0.310600;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1168, Train Accuracy: 96.5933%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1168, Train Accuracy: 96.5933%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1111, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 5\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1111, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/60000 (11%)], Train Loss: 0.286016;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)], Train Loss: 0.286016;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)], Train Loss: 0.471843;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)], Train Loss: 0.471843;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/60000 (32%)], Train Loss: 0.430391;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)], Train Loss: 0.430391;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)], Train Loss: 0.472487;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)], Train Loss: 0.472487;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32000/60000 (53%)], Train Loss: 0.387425;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)], Train Loss: 0.387425;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)], Train Loss: 0.405127;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)], Train Loss: 0.405127;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44800/60000 (75%)], Train Loss: 0.623517;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)], Train Loss: 0.623517;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)], Train Loss: 0.439544;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)], Train Loss: 0.439544;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [57600/60000 (96%)], Train Loss: 0.577280;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)], Train Loss: 0.577280;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1053, Train Accuracy: 96.8933%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1053, Train Accuracy: 96.8933%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1003, Test Accuracy: 96.9800%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1003, Test Accuracy: 96.9800%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 6\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/60000 (11%)], Train Loss: 0.374831;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [6400/60000 (11%)], Train Loss: 0.374831;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/60000 (21%)], Train Loss: 0.291880;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [12800/60000 (21%)], Train Loss: 0.291880;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/60000 (32%)], Train Loss: 0.241920;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [19200/60000 (32%)], Train Loss: 0.241920;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/60000 (43%)], Train Loss: 0.169306;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [25600/60000 (43%)], Train Loss: 0.169306;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [32000/60000 (53%)], Train Loss: 0.211547;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [32000/60000 (53%)], Train Loss: 0.211547;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [38400/60000 (64%)], Train Loss: 0.255479;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [38400/60000 (64%)], Train Loss: 0.255479;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [44800/60000 (75%)], Train Loss: 0.531673;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [44800/60000 (75%)], Train Loss: 0.531673;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/60000 (85%)], Train Loss: 0.200672;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [51200/60000 (85%)], Train Loss: 0.200672;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [57600/60000 (96%)], Train Loss: 0.578127;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [57600/60000 (96%)], Train Loss: 0.578127;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1158, Train Accuracy: 96.6683%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1158, Train Accuracy: 96.6683%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1127, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 7\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1127, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [6400/60000 (11%)], Train Loss: 0.272450;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [6400/60000 (11%)], Train Loss: 0.272450;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/60000 (21%)], Train Loss: 0.370454;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [12800/60000 (21%)], Train Loss: 0.370454;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19200/60000 (32%)], Train Loss: 0.354826;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [19200/60000 (32%)], Train Loss: 0.354826;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/60000 (43%)], Train Loss: 0.359810;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [25600/60000 (43%)], Train Loss: 0.359810;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [32000/60000 (53%)], Train Loss: 0.477809;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [32000/60000 (53%)], Train Loss: 0.477809;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [38400/60000 (64%)], Train Loss: 0.393140;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [38400/60000 (64%)], Train Loss: 0.393140;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [44800/60000 (75%)], Train Loss: 0.414792;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [44800/60000 (75%)], Train Loss: 0.414792;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [51200/60000 (85%)], Train Loss: 0.352223;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [51200/60000 (85%)], Train Loss: 0.352223;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [57600/60000 (96%)], Train Loss: 0.453954;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [57600/60000 (96%)], Train Loss: 0.453954;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1193, Train Accuracy: 96.5733%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1193, Train Accuracy: 96.5733%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1142, Test Accuracy: 96.8900%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 8\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1142, Test Accuracy: 96.8900%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [6400/60000 (11%)], Train Loss: 0.418022;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [6400/60000 (11%)], Train Loss: 0.418022;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/60000 (21%)], Train Loss: 0.621582;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [12800/60000 (21%)], Train Loss: 0.621582;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [19200/60000 (32%)], Train Loss: 0.388332;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [19200/60000 (32%)], Train Loss: 0.388332;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/60000 (43%)], Train Loss: 0.320884;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [25600/60000 (43%)], Train Loss: 0.320884;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [32000/60000 (53%)], Train Loss: 0.344557;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [32000/60000 (53%)], Train Loss: 0.344557;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [38400/60000 (64%)], Train Loss: 0.512480;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [38400/60000 (64%)], Train Loss: 0.512480;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [44800/60000 (75%)], Train Loss: 0.401572;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [44800/60000 (75%)], Train Loss: 0.401572;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [51200/60000 (85%)], Train Loss: 0.240122;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [51200/60000 (85%)], Train Loss: 0.240122;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [57600/60000 (96%)], Train Loss: 0.562563;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [57600/60000 (96%)], Train Loss: 0.562563;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1084, Train Accuracy: 96.9750%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1084, Train Accuracy: 96.9750%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1084, Test Accuracy: 96.9800%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 9\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1084, Test Accuracy: 96.9800%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [6400/60000 (11%)], Train Loss: 0.213717;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [6400/60000 (11%)], Train Loss: 0.213717;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/60000 (21%)], Train Loss: 0.291443;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [12800/60000 (21%)], Train Loss: 0.291443;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [19200/60000 (32%)], Train Loss: 0.145249;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [19200/60000 (32%)], Train Loss: 0.145249;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/60000 (43%)], Train Loss: 0.249535;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [25600/60000 (43%)], Train Loss: 0.249535;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [32000/60000 (53%)], Train Loss: 0.260525;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [32000/60000 (53%)], Train Loss: 0.260525;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [38400/60000 (64%)], Train Loss: 0.193871;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [38400/60000 (64%)], Train Loss: 0.193871;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [44800/60000 (75%)], Train Loss: 0.242406;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [44800/60000 (75%)], Train Loss: 0.242406;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [51200/60000 (85%)], Train Loss: 0.334210;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [51200/60000 (85%)], Train Loss: 0.334210;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [57600/60000 (96%)], Train Loss: 0.354392;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [57600/60000 (96%)], Train Loss: 0.354392;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1177, Train Accuracy: 96.5450%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1177, Train Accuracy: 96.5450%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1131, Test Accuracy: 96.7400%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 10\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1131, Test Accuracy: 96.7400%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [6400/60000 (11%)], Train Loss: 0.354139;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [6400/60000 (11%)], Train Loss: 0.354139;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/60000 (21%)], Train Loss: 0.247433;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [12800/60000 (21%)], Train Loss: 0.247433;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19200/60000 (32%)], Train Loss: 0.364003;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [19200/60000 (32%)], Train Loss: 0.364003;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/60000 (43%)], Train Loss: 0.495538;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [25600/60000 (43%)], Train Loss: 0.495538;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [32000/60000 (53%)], Train Loss: 0.310818;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [32000/60000 (53%)], Train Loss: 0.310818;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [38400/60000 (64%)], Train Loss: 0.837480;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [38400/60000 (64%)], Train Loss: 0.837480;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [44800/60000 (75%)], Train Loss: 0.347442;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [44800/60000 (75%)], Train Loss: 0.347442;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [51200/60000 (85%)], Train Loss: 0.621707;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [51200/60000 (85%)], Train Loss: 0.621707;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [57600/60000 (96%)], Train Loss: 0.238564;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [57600/60000 (96%)], Train Loss: 0.238564;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1097, Train Accuracy: 96.7817%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1097, Train Accuracy: 96.7817%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1032, Test Accuracy: 96.9700%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1032, Test Accuracy: 96.9700%;\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\n",
      "2023-01-03 09:38:48 Uploading - Uploading generated training model\u001b[34m2023-01-03 09:38:45,087 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:38:45,087 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:38:45,088 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-01-03 09:38:59 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n",
      "Training seconds: 308\n",
      "Billable seconds: 308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_channels- 10  optimizer- sgd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-01-03-09-39-31-264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03 09:39:32 Starting - Starting the training job...\n",
      "2023-01-03 09:39:48 Starting - Preparing the instances for training.........\n",
      "2023-01-03 09:41:25 Downloading - Downloading input data\n",
      "2023-01-03 09:41:25 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,083 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,085 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,087 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,096 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,098 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,290 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,292 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,303 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,305 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,316 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,317 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,327 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_channels\": 10,\n",
      "        \"optimizer\": \"sgd\",\n",
      "        \"region\": \"ap-southeast-2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-01-03-09-39-31-264\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-09-39-31-264/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_channels\":10,\"optimizer\":\"sgd\",\"region\":\"ap-southeast-2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-09-39-31-264/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.c5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_channels\":10,\"optimizer\":\"sgd\",\"region\":\"ap-southeast-2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-01-03-09-39-31-264\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-2-201364840562/pytorch-training-2023-01-03-09-39-31-264/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_channels\",\"10\",\"--optimizer\",\"sgd\",\"--region\",\"ap-southeast-2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_CHANNELS=10\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=sgd\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=ap-southeast-2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 10 --optimizer sgd --region ap-southeast-2\u001b[0m\n",
      "\u001b[34m2023-01-03 09:41:27,881 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.122.0)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.126.0.tar.gz (654 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 654.9/654.9 kB 40.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.30 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2022.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.30->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sagemaker: filename=sagemaker-2.126.0-py2.py3-none-any.whl size=890069 sha256=789d45b268790f65f92814721bc4340828ab7bd1a7b938db73daab417a349d73\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/8d/05/18/aed11de4cbba829d865b14cc3758e697e789af1ac01245e290\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.122.0\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.122.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.122.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-2.126.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 9912422/9912422 [00:00<00:00, 117587680.15it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/28881 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 28881/28881 [00:00<00:00, 23930401.78it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1648877/1648877 [00:00<00:00, 191226328.50it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/4542 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 4542/4542 [00:00<00:00, 38254073.83it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.experiments.run:The run (experiment-run-example-5) under experiment (training-job-experiment-1672305742-bcbe) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mTraining Epoch: 1\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.435 algo-1:43 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.593 algo-1:43 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.594 algo-1:43 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.595 algo-1:43 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.595 algo-1:43 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.595 algo-1:43 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.611 algo-1:43 INFO hook.py:561] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.611 algo-1:43 INFO hook.py:561] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.611 algo-1:43 INFO hook.py:561] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.612 algo-1:43 INFO hook.py:561] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.612 algo-1:43 INFO hook.py:561] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.612 algo-1:43 INFO hook.py:561] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.612 algo-1:43 INFO hook.py:561] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.612 algo-1:43 INFO hook.py:561] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.612 algo-1:43 INFO hook.py:563] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.612 algo-1:43 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2023-01-03 09:41:35.614 algo-1:43 INFO hook.py:485] Hook is writing from the hook with pid: 43\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/60000 (11%)], Train Loss: 2.196721;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)], Train Loss: 2.196721;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)], Train Loss: 1.481765;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)], Train Loss: 1.481765;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)], Train Loss: 0.782120;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)], Train Loss: 0.782120;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)], Train Loss: 0.629781;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)], Train Loss: 0.629781;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)], Train Loss: 0.766893;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)], Train Loss: 0.766893;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)], Train Loss: 0.519505;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)], Train Loss: 0.519505;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)], Train Loss: 0.583272;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)], Train Loss: 0.583272;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)], Train Loss: 0.324233;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)], Train Loss: 0.324233;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)], Train Loss: 0.341933;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)], Train Loss: 0.341933;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.2189, Train Accuracy: 93.6050%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.2189, Train Accuracy: 93.6050%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.2045, Test Accuracy: 93.9500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.2045, Test Accuracy: 93.9500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)], Train Loss: 1.006587;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)], Train Loss: 1.006587;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)], Train Loss: 0.508417;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)], Train Loss: 0.508417;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)], Train Loss: 0.307927;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)], Train Loss: 0.307927;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)], Train Loss: 0.312856;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)], Train Loss: 0.312856;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)], Train Loss: 0.304903;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)], Train Loss: 0.304903;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)], Train Loss: 0.323135;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)], Train Loss: 0.323135;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)], Train Loss: 0.341664;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)], Train Loss: 0.341664;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)], Train Loss: 0.412401;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)], Train Loss: 0.412401;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)], Train Loss: 0.208245;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)], Train Loss: 0.208245;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1387, Train Accuracy: 95.7683%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1387, Train Accuracy: 95.7683%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1295, Test Accuracy: 96.0200%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 3\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1295, Test Accuracy: 96.0200%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/60000 (11%)], Train Loss: 0.294959;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)], Train Loss: 0.294959;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)], Train Loss: 0.237874;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)], Train Loss: 0.237874;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/60000 (32%)], Train Loss: 0.348222;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)], Train Loss: 0.348222;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)], Train Loss: 0.112146;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)], Train Loss: 0.112146;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32000/60000 (53%)], Train Loss: 0.424070;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)], Train Loss: 0.424070;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)], Train Loss: 0.231900;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)], Train Loss: 0.231900;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/60000 (75%)], Train Loss: 0.247123;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)], Train Loss: 0.247123;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)], Train Loss: 0.517267;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)], Train Loss: 0.517267;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [57600/60000 (96%)], Train Loss: 0.323532;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)], Train Loss: 0.323532;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1080, Train Accuracy: 96.7133%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1080, Train Accuracy: 96.7133%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1022, Test Accuracy: 96.7800%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 4\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1022, Test Accuracy: 96.7800%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/60000 (11%)], Train Loss: 0.148583;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)], Train Loss: 0.148583;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)], Train Loss: 0.363557;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)], Train Loss: 0.363557;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/60000 (32%)], Train Loss: 0.176086;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)], Train Loss: 0.176086;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)], Train Loss: 0.200272;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)], Train Loss: 0.200272;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32000/60000 (53%)], Train Loss: 0.381149;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)], Train Loss: 0.381149;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)], Train Loss: 0.152197;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)], Train Loss: 0.152197;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44800/60000 (75%)], Train Loss: 0.475347;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)], Train Loss: 0.475347;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)], Train Loss: 0.149382;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)], Train Loss: 0.149382;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [57600/60000 (96%)], Train Loss: 0.116603;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)], Train Loss: 0.116603;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0908, Train Accuracy: 97.2567%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0908, Train Accuracy: 97.2567%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0863, Test Accuracy: 97.2500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 5\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0863, Test Accuracy: 97.2500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/60000 (11%)], Train Loss: 0.116118;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)], Train Loss: 0.116118;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)], Train Loss: 0.336291;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)], Train Loss: 0.336291;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/60000 (32%)], Train Loss: 0.147341;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)], Train Loss: 0.147341;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)], Train Loss: 0.354664;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)], Train Loss: 0.354664;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32000/60000 (53%)], Train Loss: 0.122797;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)], Train Loss: 0.122797;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)], Train Loss: 0.375539;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)], Train Loss: 0.375539;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44800/60000 (75%)], Train Loss: 0.374760;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)], Train Loss: 0.374760;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)], Train Loss: 0.222917;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)], Train Loss: 0.222917;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [57600/60000 (96%)], Train Loss: 0.354818;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)], Train Loss: 0.354818;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0797, Train Accuracy: 97.5650%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0797, Train Accuracy: 97.5650%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0733, Test Accuracy: 97.6500%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0733, Test Accuracy: 97.6500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 6\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/60000 (11%)], Train Loss: 0.266578;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [6400/60000 (11%)], Train Loss: 0.266578;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/60000 (21%)], Train Loss: 0.364486;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [12800/60000 (21%)], Train Loss: 0.364486;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/60000 (32%)], Train Loss: 0.196769;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [19200/60000 (32%)], Train Loss: 0.196769;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/60000 (43%)], Train Loss: 0.148534;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [25600/60000 (43%)], Train Loss: 0.148534;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [32000/60000 (53%)], Train Loss: 0.227337;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [32000/60000 (53%)], Train Loss: 0.227337;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [38400/60000 (64%)], Train Loss: 0.118226;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [38400/60000 (64%)], Train Loss: 0.118226;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [44800/60000 (75%)], Train Loss: 0.316014;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [44800/60000 (75%)], Train Loss: 0.316014;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/60000 (85%)], Train Loss: 0.208127;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [51200/60000 (85%)], Train Loss: 0.208127;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [57600/60000 (96%)], Train Loss: 0.185497;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [57600/60000 (96%)], Train Loss: 0.185497;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0739, Train Accuracy: 97.7200%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0739, Train Accuracy: 97.7200%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0675, Test Accuracy: 97.9100%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 7\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0675, Test Accuracy: 97.9100%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [6400/60000 (11%)], Train Loss: 0.190629;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [6400/60000 (11%)], Train Loss: 0.190629;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/60000 (21%)], Train Loss: 0.269759;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [12800/60000 (21%)], Train Loss: 0.269759;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19200/60000 (32%)], Train Loss: 0.173283;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [19200/60000 (32%)], Train Loss: 0.173283;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/60000 (43%)], Train Loss: 0.236415;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [25600/60000 (43%)], Train Loss: 0.236415;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [32000/60000 (53%)], Train Loss: 0.157724;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [32000/60000 (53%)], Train Loss: 0.157724;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [38400/60000 (64%)], Train Loss: 0.085673;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [38400/60000 (64%)], Train Loss: 0.085673;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [44800/60000 (75%)], Train Loss: 0.236443;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [44800/60000 (75%)], Train Loss: 0.236443;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [51200/60000 (85%)], Train Loss: 0.110556;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [51200/60000 (85%)], Train Loss: 0.110556;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [57600/60000 (96%)], Train Loss: 0.252375;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [57600/60000 (96%)], Train Loss: 0.252375;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0692, Train Accuracy: 97.8817%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0692, Train Accuracy: 97.8817%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0648, Test Accuracy: 97.9600%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 8\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0648, Test Accuracy: 97.9600%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [6400/60000 (11%)], Train Loss: 0.239406;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [6400/60000 (11%)], Train Loss: 0.239406;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/60000 (21%)], Train Loss: 0.222502;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [12800/60000 (21%)], Train Loss: 0.222502;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [19200/60000 (32%)], Train Loss: 0.243818;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [19200/60000 (32%)], Train Loss: 0.243818;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/60000 (43%)], Train Loss: 0.191509;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [25600/60000 (43%)], Train Loss: 0.191509;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [32000/60000 (53%)], Train Loss: 0.268245;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [32000/60000 (53%)], Train Loss: 0.268245;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [38400/60000 (64%)], Train Loss: 0.227699;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [38400/60000 (64%)], Train Loss: 0.227699;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [44800/60000 (75%)], Train Loss: 0.214783;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [44800/60000 (75%)], Train Loss: 0.214783;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [51200/60000 (85%)], Train Loss: 0.259731;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [51200/60000 (85%)], Train Loss: 0.259731;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [57600/60000 (96%)], Train Loss: 0.324707;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [57600/60000 (96%)], Train Loss: 0.324707;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0661, Train Accuracy: 97.9367%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0661, Train Accuracy: 97.9367%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0620, Test Accuracy: 98.0300%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 9\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0620, Test Accuracy: 98.0300%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [6400/60000 (11%)], Train Loss: 0.247424;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [6400/60000 (11%)], Train Loss: 0.247424;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/60000 (21%)], Train Loss: 0.212408;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [12800/60000 (21%)], Train Loss: 0.212408;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [19200/60000 (32%)], Train Loss: 0.056094;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [19200/60000 (32%)], Train Loss: 0.056094;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/60000 (43%)], Train Loss: 0.173286;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [25600/60000 (43%)], Train Loss: 0.173286;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [32000/60000 (53%)], Train Loss: 0.196937;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [32000/60000 (53%)], Train Loss: 0.196937;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [38400/60000 (64%)], Train Loss: 0.120318;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [38400/60000 (64%)], Train Loss: 0.120318;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [44800/60000 (75%)], Train Loss: 0.092031;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [44800/60000 (75%)], Train Loss: 0.092031;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [51200/60000 (85%)], Train Loss: 0.242876;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [51200/60000 (85%)], Train Loss: 0.242876;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [57600/60000 (96%)], Train Loss: 0.078139;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [57600/60000 (96%)], Train Loss: 0.078139;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0661, Train Accuracy: 97.9417%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0661, Train Accuracy: 97.9417%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0633, Test Accuracy: 98.0700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 10\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0633, Test Accuracy: 98.0700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [6400/60000 (11%)], Train Loss: 0.146423;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [6400/60000 (11%)], Train Loss: 0.146423;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/60000 (21%)], Train Loss: 0.064179;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [12800/60000 (21%)], Train Loss: 0.064179;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19200/60000 (32%)], Train Loss: 0.131335;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [19200/60000 (32%)], Train Loss: 0.131335;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/60000 (43%)], Train Loss: 0.113795;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [25600/60000 (43%)], Train Loss: 0.113795;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [32000/60000 (53%)], Train Loss: 0.194988;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [32000/60000 (53%)], Train Loss: 0.194988;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [38400/60000 (64%)], Train Loss: 0.117052;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [38400/60000 (64%)], Train Loss: 0.117052;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [44800/60000 (75%)], Train Loss: 0.070461;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [44800/60000 (75%)], Train Loss: 0.070461;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [51200/60000 (85%)], Train Loss: 0.314221;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [51200/60000 (85%)], Train Loss: 0.314221;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [57600/60000 (96%)], Train Loss: 0.167088;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [57600/60000 (96%)], Train Loss: 0.167088;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.0592, Train Accuracy: 98.1417%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.0592, Train Accuracy: 98.1417%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0570, Test Accuracy: 98.2000%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0570, Test Accuracy: 98.2000%;\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:46:37,234 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:46:37,235 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-01-03 09:46:37,235 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-01-03 09:47:38 Uploading - Uploading generated training model\n",
      "2023-01-03 09:47:38 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n",
      "Training seconds: 393\n",
      "Billable seconds: 393\n",
      "CPU times: user 6.06 s, sys: 474 ms, total: 6.53 s\n",
      "Wall time: 29min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Start training job with experiment setting\n",
    "num=1\n",
    "hidden_channels=[5,10]\n",
    "optimizer=[\"adam\", \"sgd\"]\n",
    "\n",
    "for h in hidden_channels:\n",
    "    for j in optimizer:\n",
    "        num=num+1\n",
    "        run_name_n=run_name+'-'+str(num)\n",
    "        with Run(experiment_name=experiment_name, run_name=run_name_n) as run:\n",
    "            print(\"hidden_channels-\", h, \" optimizer-\", j)\n",
    "            estmator = PyTorch(\n",
    "                entry_point=\"./script/mnist.py\",\n",
    "                role=role,\n",
    "                model_dir=False,\n",
    "                framework_version=\"1.12\",\n",
    "                py_version=\"py38\",\n",
    "                instance_type=\"ml.c5.xlarge\",\n",
    "                instance_count=1,\n",
    "                hyperparameters={\"epochs\": 10, \"hidden_channels\": h, \"optimizer\": j, \"region\": region},\n",
    "                keep_alive_period_in_seconds=3600,\n",
    "            )\n",
    "\n",
    "            estmator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6da7121a-96d6-4634-9968-cefea4dac2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-experiment-example-1674025070-8ceb\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms #to download and transform mnist dataset\n",
    "import torch\n",
    "\n",
    "from sagemaker.session import Session  #to Manage interactions with the Amazon SageMaker APIs and any other AWS services needed.\n",
    "from sagemaker.experiments.run import Run, load_run # A collection of parameters, metrics, and artifacts to create a ML model.\n",
    "from sagemaker.utils import unique_name_from_base #adding unique id for a name\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "experiment_name = unique_name_from_base(\"local-experiment-example\") #unique \n",
    "run_name = \"experiment-run\"\n",
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259c4fca-5128-4213-8065-cb68bd50b973",
   "metadata": {
    "tags": []
   },
   "source": [
    "Checking the SageMaker Experiments UI, you can observe the Experiment run, populated with the metrics and parameters logged. We can also see the automatically generated outputs for the model data\n",
    "\n",
    "\n",
    "<img src=\"images/sm_training_exp_overview.png\" width=\"100%\" style=\"float: left;\" />\n",
    "<img src=\"images/sm_training_inputs.png\" width=\"100%\" style=\"float: left;\" />\n",
    "<img src=\"images/sm_training_parameters.png\" width=\"100%\" style=\"float: left;\" />\n",
    "<img src=\"images/sm_training_metrics.png\" width=\"100%\" style=\"float: left;\" />\n",
    "<img src=\"images/sm_training_outputs.png\" width=\"100%\" style=\"float: left;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7aafc488-b3bc-4032-8772-d943e3491a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name=\"hpo-training-experiment-pytorch\"\n",
    "run_name=\"hpo-test\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8e4f80b-01ec-415b-8e80-507bbeadc402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Start training job with experiment setting\n",
    "\n",
    "estimator2 = PyTorch(\n",
    "    entry_point=\"./script/mnist.py\",\n",
    "    role=role,\n",
    "    model_dir=False,\n",
    "    framework_version=\"1.12\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    instance_count=1,\n",
    "    hyperparameters={\"epochs\": 10, \"hidden_channels\": 5, \"optimizer\": \"adam\", \"region\": region},\n",
    "    #keep_alive_period_in_seconds=3600,\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67a53179-1c18-417b-bcb3-340fc9212471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter, IntegerParameter,CategoricalParameter, HyperparameterTuner\n",
    "\n",
    "\n",
    "objective_metric_name = \"validation_acc\"\n",
    "objective_type = \"Maximize\"\n",
    "metric_definitions = [{\"Name\": \"validation_acc\", \"Regex\": \"Accuracy: {:.4f}%\"}]\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"hidden_channels\": IntegerParameter(3,10),\n",
    "    \"optimizer\": CategoricalParameter([\"adam\", \"sgd\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16d1dda2-30ea-46a3-9be4-d43bfa6891a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating hyperparameter tuning job with name: pytorch-training-230118-0703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................................................................*\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for HyperParameterTuning job pytorch-training-230118-0703: Failed. Reason: No training job succeeded after 5 attempts. For additional details, please take a look at the training job failures by listing training jobs for the hyperparameter tuning job.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_StepArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/tuner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, job_name, include_cls_metadata, estimator_kwargs, wait, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_tuning_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_with_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_cls_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/tuner.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1901\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m         \u001b[0;34m\"\"\"Placeholder docstring.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1903\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_tuning_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_tuning_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   3553\u001b[0m         \u001b[0mRaises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3554\u001b[0m             \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapacityError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mhyperparameter\u001b[0m \u001b[0mtuning\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mfails\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mCapacityError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3555\u001b[0;31m             \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedStatusException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mhyperparameter\u001b[0m \u001b[0mtuning\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3556\u001b[0m         \"\"\"\n\u001b[1;32m   3557\u001b[0m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_tuning_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3642\u001b[0m                 raise exceptions.CapacityError(\n\u001b[1;32m   3643\u001b[0m                     \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3644\u001b[0;31m                     \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3645\u001b[0m                     \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3646\u001b[0m                 )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for HyperParameterTuning job pytorch-training-230118-0703: Failed. Reason: No training job succeeded after 5 attempts. For additional details, please take a look at the training job failures by listing training jobs for the hyperparameter tuning job."
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "\n",
    "\n",
    "with Run(\n",
    "    experiment_name=experiment_name,\n",
    "    run_name=unique_name_from_base(\"hpo-test\"),\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "    hyperparam_tuner = HyperparameterTuner(\n",
    "        estimator = estimator2,\n",
    "        early_stopping_type = \"Auto\",\n",
    "        objective_metric_name = objective_metric_name,\n",
    "        metric_definitions = metric_definitions,\n",
    "        strategy = \"Bayesian\",\n",
    "        objective_type = \"Maximize\",\n",
    "        max_jobs = 10,\n",
    "        max_parallel_jobs = 1,\n",
    "        hyperparameter_ranges = hyperparameter_ranges\n",
    "    )\n",
    "    \n",
    "    hyperparam_tuner.fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bcf894-5700-43db-ab72-0c6225900bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "forced_instance_type": "ml.t3.medium",
  "forced_lcc_arn": "",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/pytorch-1.10-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
